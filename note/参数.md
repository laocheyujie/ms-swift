### model_info
ModelInfo(
    model_type='glm4_5', 
    model_dir='/models/ZhipuAI/GLM-4.5-Air', 
    torch_dtype=torch.bfloat16, 
    max_model_len=131072, 
    quant_method=None, 
    quant_bits=None, 
    rope_scaling=None, 
    is_moe_model=True, 
    config=None, 
    task_type='causal_lm', 
    num_labels=None
)


### model_meta
ModelMeta(
    model_type='glm4_5', 
    model_groups=[
        ModelGroup(
            models=[
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5-Air-Base', 
                    hf_model_id='zai-org/GLM-4.5-Air-Base', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                ), 
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5-Air', 
                    hf_model_id='zai-org/GLM-4.5-Air', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                ), 
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5-Air-FP8', 
                    hf_model_id='zai-org/GLM-4.5-Air-FP8', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                ), 
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5-Base', 
                    hf_model_id='zai-org/GLM-4.5-Base', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                ), 
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5', 
                    hf_model_id='zai-org/GLM-4.5', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                ), 
                Model(
                    ms_model_id='ZhipuAI/GLM-4.5-FP8', 
                    hf_model_id='zai-org/GLM-4.5-FP8', 
                    model_path=None, 
                    ms_revision=None, 
                    hf_revision=None
                )
            ], 
            ignore_patterns=None, 
            requires=None, 
            tags=[]
        )
    ], 
    template='glm4_5', 
    get_function=<function get_model_tokenizer_with_flash_attn at 0x7fa6287140e0>, 
    model_arch=None, 
    architectures=['Glm4MoeForCausalLM'], 
    additional_saved_files=[], 
    torch_dtype=None, 
    is_multimodal=False, 
    is_reward=False, 
    task_type=None, 
    ignore_patterns=None, 
    requires=['transformers>=4.54'], 
    tags=[]
)

### MODEL_MAPPING
{
    'emu3_gen': ModelMeta(
        model_type='emu3_gen', 
        model_groups=[
            ModelGroup(
                models=[
                    Model(
                        ms_model_id='BAAI/Emu3-Gen', 
                        hf_model_id='BAAI/Emu3-Gen', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    )
                ], 
                ignore_patterns=None, 
                requires=None, 
                tags=[]
            )
        ], 
        template='emu3_gen', 
        get_function=<function get_model_tokenizer_emu3_gen at 0x7f01e9d03e20>, 
        model_arch=MultiModelKeys(
            arch_name='emu3_chat', 
            embedding=None, 
            module_list=None, 
            lm_head=None, 
            q_proj=None, 
            k_proj=None, 
            v_proj=None, 
            o_proj=None, 
            attention=None, 
            mlp=None, 
            down_proj=None, 
            qkv_proj=None, 
            qk_proj=None, 
            qa_proj=None, 
            qb_proj=None, 
            kv_proj=None, 
            kva_proj=None, 
            kvb_proj=None, 
            language_model=['model'], 
            aligner=[], 
            vision_tower=[], 
            generator=[]
        ), 
        architectures=['Emu3ForCausalLM'], 
        additional_saved_files=[], 
        torch_dtype=None, 
        is_multimodal=True, 
        is_reward=False, 
        task_type=None, 
        ignore_patterns=None, 
        requires=[], 
        tags=['t2i']
    ),
    ...,
    'glm4_5': ModelMeta(
        model_type='glm4_5', 
        model_groups=[
            ModelGroup(
                models=[
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5-Air-Base', 
                        hf_model_id='zai-org/GLM-4.5-Air-Base', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    ), 
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5-Air', 
                        hf_model_id='zai-org/GLM-4.5-Air', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    ), 
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5-Air-FP8', 
                        hf_model_id='zai-org/GLM-4.5-Air-FP8', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    ), 
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5-Base', 
                        hf_model_id='zai-org/GLM-4.5-Base', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    ), 
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5', 
                        hf_model_id='zai-org/GLM-4.5', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    ), 
                    Model(
                        ms_model_id='ZhipuAI/GLM-4.5-FP8', 
                        hf_model_id='zai-org/GLM-4.5-FP8', 
                        model_path=None, 
                        ms_revision=None, 
                        hf_revision=None
                    )
                ], 
                ignore_patterns=None, 
                requires=None, 
                tags=[]
            )
        ], 
        template='glm4_5', 
        get_function=<function get_model_tokenizer_with_flash_attn at 0x7f023fdb8220>, 
        model_arch=None, 
        architectures=['Glm4MoeForCausalLM'], 
        additional_saved_files=[], 
        torch_dtype=None, 
        is_multimodal=False, 
        is_reward=False, 
        task_type=None, 
        ignore_patterns=None, 
        requires=['transformers>=4.54'], 
        tags=[]
    ),
    ...
}


### MEGATRON_MODEL_MAPPING
{
    'gpt': MegatronModelMeta(
        megatron_model_type='gpt', 
        model_types=[
            'qwen2', 
            'qwen2_5', 
            'qwq', 
            'qwq_preview', 
            'qwen2_5_math', 
            'llama', 
            'llama3', 
            'llama3_1', 
            'llama3_2', 
            'longwriter_llama3_1', 
            'codefuse_codellama', 
            'marco_o1', 
            'deepseek', 
            'deepseek_r1_distill', 
            'yi', 
            'yi_coder', 
            'sus', 
            'skywork_o1', 
            'openbuddy_llama', 
            'openbuddy_llama3', 
            'megrez', 
            'reflection', 
            'numina', 
            'ziya', 
            'mengzi3', 
            'qwen3', 
            'qwen3_thinking', 
            'qwen2_moe', 
            'qwen3_moe', 
            'qwen3_moe_thinking', 
            'internlm3', 
            'mimo', 
            'mimo_rl', 
            'moonlight', 
            'deepseek_moe', 
            'deepseek_v2', 
            'deepseek_v2_5', 
            'deepseek_r1', 
            'dots1', 
            'ernie', 
            'glm4_5'
        ], 
        model_provider=<function model_provider at 0x7fcb740feac0>, 
        convert_hf_config=<function convert_gpt_hf_config at 0x7fcb740fdd00>, 
        convert_mcore2hf=<function convert_mcore2hf at 0x7fcb740fe980>, 
        convert_hf2mcore=<function convert_hf2mcore at 0x7fcb740fe520>, 
        extra_args_provider=None
    )
}

### _MODEL_META_MAPPING
{
    'qwen2': 'gpt', 
    'qwen2_5': 'gpt', 
    'qwq': 'gpt', 
    'qwq_preview': 'gpt', 
    'qwen2_5_math': 'gpt', 
    'llama': 'gpt', 
    'llama3': 'gpt', 
    'llama3_1': 'gpt', 
    'llama3_2': 'gpt', 
    'longwriter_llama3_1': 'gpt', 
    'codefuse_codellama': 'gpt', 
    'marco_o1': 'gpt', 
    'deepseek': 'gpt', 
    'deepseek_r1_distill': 'gpt', 
    'yi': 'gpt', 
    'yi_coder': 'gpt', 
    'sus': 'gpt', 
    'skywork_o1': 'gpt', 
    'openbuddy_llama': 'gpt', 
    'openbuddy_llama3': 'gpt', 
    'megrez': 'gpt', 
    'reflection': 'gpt', 
    'numina': 'gpt', 
    'ziya': 'gpt', 
    'mengzi3': 'gpt', 
    'qwen3': 'gpt', 
    'qwen3_thinking': 'gpt', 
    'qwen2_moe': 'gpt', 
    'qwen3_moe': 'gpt', 
    'qwen3_moe_thinking': 'gpt', 
    'internlm3': 'gpt', 
    'mimo': 'gpt', 
    'mimo_rl': 'gpt', 
    'moonlight': 'gpt', 
    'deepseek_moe': 'gpt', 
    'deepseek_v2': 'gpt', 
    'deepseek_v2_5': 'gpt', 
    'deepseek_r1': 'gpt', 
    'dots1': 'gpt', 
    'ernie': 'gpt', 
    'glm4_5': 'gpt'
}


### HuggingFace config
Glm4MoeConfig {
  "architectures": [
    "Glm4MoeForCausalLM"
  ],
  "attention_bias": true,
  "attention_dropout": 0.0,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "first_k_dense_replace": 1,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 10944,
  "max_position_embeddings": 131072,
  "model_type": "glm4_moe",
  "moe_intermediate_size": 1408,
  "n_group": 1,
  "n_routed_experts": 128,
  "n_shared_experts": 1,
  "norm_topk_prob": true,
  "num_attention_heads": 96,
  "num_experts_per_tok": 8,
  "num_hidden_layers": 46,
  "num_key_value_heads": 8,
  "num_nextn_predict_layers": 1,
  "pad_token_id": 151329,
  "partial_rotary_factor": 0.5,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 1000000,
  "routed_scaling_factor": 1.0,
  "tie_word_embeddings": false,
  "topk_group": 1,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.54.1",
  "use_cache": true,
  "use_qk_norm": false,
  "vocab_size": 151552
}

### Megatron config
{
    'num_layers': 46, 
    'hidden_size': 4096, 
    'ffn_hidden_size': 10944, 
    'num_attention_heads': 96, 
    'num_query_groups': 8, 
    'max_position_embeddings': 131072, 
    'norm_epsilon': 1e-05, 
    'rotary_base': 1000000, 
    'padded_vocab_size': 151552, 
    'attention_dropout': 0.0, 
    'untie_embeddings_and_output_weights': True, 
    'swiglu': True, 
    'add_qkv_bias': True, 
    'kv_channels': 128, 
    'architectures': 'Glm4MoeForCausalLM', 
    'moe_ffn_hidden_size': 1408, 
    'moe_router_topk': 8, 
    'num_experts': 128, 
    'moe_router_pre_softmax': False, 
    'moe_router_topk_scaling_factor': 1.0, 
    'qk_layernorm': False, 
    'partial_rotary_factor': 0.5, 
    'moe_router_score_function': 'sigmoid', 
    'moe_layer_freq': '[0]*1+[1]*45', 
    'moe_router_enable_expert_bias': True, 
    'moe_shared_expert_intermediate_size': 1408
}

### new_args Megatron 标准参数
[
    '--seed', '42', 
    '--micro-batch-size', '1', 
    '--global-batch-size', '16', 
    '--recompute-granularity', 'full', 
    '--recompute-method', 'uniform', 
    '--recompute-num-layers', '1', 
    '--recompute-modules', 'core_attn', 
    '--log-interval', '5', 
    '--tensorboard-dir', '/models/megatron_output/GLM-4.5-Air-Debug/v0-20250827-140714/runs', 
    '--cross-entropy-loss-fusion', 
    '--cross-entropy-fusion-impl', 'native', 
    '--calculate-per-token-loss', 
    '--attention-backend', 'flash', 
    '--optimizer', 'adam', 
    '--optimizer-offload-fraction', '1.0', 
    '--main-grads-dtype', 'fp32', 
    '--main-params-dtype', 'fp32', 
    '--exp-avg-dtype', 'fp32', 
    '--exp-avg-sq-dtype', 'fp32', 
    '--dataloader-type', 'cyclic', 
    '--manual-gc-interval', '0', 
    '--lr', '0.0001', 
    '--lr-decay-style', 'cosine', 
    '--lr-warmup-iters', '0', 
    '--lr-warmup-fraction', '0.05', 
    '--min-lr', '1e-05', 
    '--weight-decay', '0.1', 
    '--clip-grad', '1.0', 
    '--adam-beta1', '0.9', 
    '--adam-beta2', '0.95', 
    '--adam-eps', '1e-08', 
    '--sgd-momentum', '0.9', 
    '--save', '/models/megatron_output/GLM-4.5-Air-Debug/v0-20250827-140714', 
    '--save-interval', '500', 
    '--no-save-optim', 
    '--no-save-rng', 
    '--load', '/models/ZhipuAI/GLM-4.5-Air-mcore', 
    '--finetune', 
    '--ckpt-format', 'torch_dist', 
    '--no-initialization', 
    '--auto-detect-ckpt-format', 
    '--exit-on-missing-checkpoint', 
    '--distributed-backend', 'nccl', 
    '--local-rank', '0', 
    '--use-distributed-optimizer', 
    '--tensor-model-parallel-size', '4', 
    '--pipeline-model-parallel-size', '2', 
    '--sequence-parallel', 
    '--context-parallel-size', '2', 
    '--distributed-timeout-minutes', '300000', 
    '--num-layers', '46', 
    '--hidden-size', '4096', 
    '--ffn-hidden-size', '10944', 
    '--num-attention-heads', '96', 
    '--group-query-attention', 
    '--num-query-groups', '8', 
    '--max-position-embeddings', '131072', 
    '--position-embedding-type', 'rope', 
    '--rotary-base', '1000000', 
    '--rotary-percent', '1.0', 
    '--normalization', 'RMSNorm', 
    '--norm-epsilon', '1e-05', 
    '--swiglu', 
    '--untie-embeddings-and-output-weights', 
    '--disable-bias-linear', 
    '--add-qkv-bias', 
    '--attention-dropout', '0.0', 
    '--hidden-dropout', '0.0', 
    '--kv-channels', '128', 
    '--transformer-impl', 'transformer_engine', 
    '--num-experts', '128', 
    '--moe-layer-freq', '[0]*1+[1]*45', 
    '--moe-ffn-hidden-size', '1408', 
    '--moe-shared-expert-intermediate-size', '1408', 
    '--moe-router-topk', '8', 
    '--moe-router-dtype', 'fp32', 
    '--moe-router-score-function', 'sigmoid', 
    '--moe-router-bias-update-rate', '0.001', 
    '--moe-router-enable-expert-bias', 
    '--moe-router-topk-scaling-factor', '1.0', 
    '--moe-router-load-balancing-type', 'aux_loss', 
    '--expert-model-parallel-size', '4', 
    '--expert-tensor-parallel-size', '1', 
    '--moe-token-dispatcher-type', 'alltoall', 
    '--moe-grouped-gemm', 
    '--moe-permute-fusion', 
    '--moe-aux-loss-coeff', '0.001', 
    '--moe-shared-expert-overlap', 
    '--moe-token-drop-policy', 'probs', 
    '--kv-lora-rank', '32', 
    '--qk-head-dim', '128', 
    '--qk-pos-emb-head-dim', '64', 
    '--fp8-recipe', 'delayed', 
    '--fp8-amax-history-len', '1024', 
    '--fp8-amax-compute-algo', 'max', 
    '--bf16', 
    '--attention-softmax-in-fp32', 
    '--tensorboard-log-interval', '1', 
    '--tensorboard-queue-size', '50', 
    '--log-timers-to-tensorboard', 
    '--log-validation-ppl-to-tensorboard', 
    '--log-memory-to-tensorboard', 
    '--eval-iters', '-1', 
    '--eval-interval', '200', 
    '--seq-length', '4096', 
    '--num-workers', '8'
]

### extra_args 非 Megatron 标准参数
{
    'model': '/models/ZhipuAI/GLM-4.5-Air', 
    'model_type': 'glm4_5', 
    'model_revision': None, 
    'task_type': 'causal_lm', 
    'torch_dtype': torch.bfloat16, 
    'attn_impl': None, 
    'new_special_tokens': [], 
    'num_labels': None, 
    'problem_type': None, 
    'rope_scaling': None, 
    'device_map': None, 
    'max_memory': {}, 
    'max_model_len': None, 
    'local_repo_path': None, 
    'init_strategy': None, 
    'template': 'glm4_5', 
    'system': None, 
    'max_length': 4096, 
    'truncation_strategy': 'delete', 
    'max_pixels': None, 
    'agent_template': None, 
    'norm_bbox': None, 
    'use_chat_template': True, 
    'padding_free': True, 
    'padding_side': 'right', 
    'sequence_parallel_size': 2, 
    'response_prefix': None, 
    'template_backend': 'swift', 
    'dataset': ['/datasets/debug.jsonl'], 
    'val_dataset': [], 
    'split_dataset_ratio': 0.1, 
    'data_seed': 42, 
    'dataset_num_proc': 8, 
    'load_from_cache_file': True, 
    'dataset_shuffle': True, 
    'val_dataset_shuffle': False, 
    'streaming': False, 
    'interleave_prob': None, 
    'stopping_strategy': 'first_exhausted', 
    'shuffle_buffer_size': 1000, 
    'download_mode': 'reuse_dataset_if_exists', 
    'columns': {}, 
    'strict': False, 
    'remove_unused_columns': True, 
    'model_name': None, 
    'model_author': None, 
    'custom_dataset_info': [], 
    'quant_method': None, 
    'quant_bits': None, 
    'hqq_axis': None, 
    'bnb_4bit_compute_dtype': torch.bfloat16, 
    'bnb_4bit_quant_type': 'nf4', 
    'bnb_4bit_use_double_quant': True, 
    'bnb_4bit_quant_storage': None, 
    'max_new_tokens': None, 
    'temperature': None, 
    'top_k': None, 
    'top_p': None, 
    'repetition_penalty': None, 
    'num_beams': 1, 
    'stream': False, 
    'stop_words': [], 
    'logprobs': False, 
    'top_logprobs': None, 
    'ckpt_dir': '/models/ZhipuAI/GLM-4.5-Air-mcore', 
    'lora_modules': [], 
    'tuner_backend': 'peft', 
    'train_type': 'lora', 
    'adapters': [], 
    'external_plugins': [], 
    'model_kwargs': {}, 
    'load_args': True, 
    'load_data_args': False, 
    'packing': True, 
    'packing_length': 4096, 
    'lazy_tokenize': False, 
    'cached_dataset': [], 
    'custom_register_path': [], 
    'use_hf': False, 
    'hub_token': None, 
    'ddp_timeout': 18000000, 
    'ddp_backend': None, 
    'ignore_args_error': False, 
    'use_swift_lora': False, 
    'freeze_parameters': [], 
    'freeze_parameters_regex': None, 
    'freeze_parameters_ratio': 0.0, 
    'trainable_parameters': [], 
    'trainable_parameters_regex': None, 
    'adapter_load': None, 
    'target_modules': ['all-linear'], 
    'target_regex': None, 
    'modules_to_save': [], 
    'lora_rank': 16, 
    'lora_alpha': 32, 
    'lora_dropout': 0.05, 
    'lora_bias': 'none', 
    'lora_dtype': None, 
    'use_rslora': False, 
    'ref_load': None, 
    'beta': 0.1, 
    'rpo_alpha': None, 
    'reference_free': False, 
    'label_smoothing': 0.0, 
    'f_divergence_type': 'reverse_kl', 
    'loss_type': 'sigmoid', 
    'padded_vocab_size': 151552, 
    'initialize_embedding': False, 
    'mlp_padding_free': False, 
    'dataloader_persistent_workers': True, 
    'dataloader_prefetch_factor': 10, 
    'architectures': 'Glm4MoeForCausalLM', 
    'max_epochs': 8, 
    'enable_dft_loss': False, 
    'original_max_position_embeddings': None, 
    'partial_rotary_factor': 0.5, 
    'use_shared_expert_gate': False, 
    'add_version': True
}


### template_kwargs
{
    'default_system': None, 
    'max_length': 4096, 
    'truncation_strategy': 'raise', 
    'max_pixels': None, 
    'agent_template': None, 
    'norm_bbox': None, 
    'use_chat_template': True, 
    'remove_unused_columns': True, 
    'padding_free': True, 
    'padding_side': 'right', 
    'loss_scale': 'default', 
    'sequence_parallel_size': 2, 
    'response_prefix': None, 
    'template_backend': 'swift'
}


### argv (命令行参数)
[
    '--load', '/models/ZhipuAI/GLM-4.5-Air-mcore', 
    '--dataset', '/datasets/debug.jsonl', 
    '--train_type', 'lora', 
    '--lora_rank', '16', 
    '--lora_alpha', '32', 
    '--target_modules', 'all-linear', 
    '--split_dataset_ratio', '0.1', 
    '--pipeline_model_parallel_size', '2', 
    '--tensor_model_parallel_size', '4', 
    '--expert_tensor_parallel_size', '1', 
    '--expert_model_parallel_size', '4', 
    '--context_parallel_size', '2', 
    '--sequence_parallel', 'true', 
    '--moe_permute_fusion', 'true', 
    '--moe_grouped_gemm', 'true', 
    '--moe_shared_expert_overlap', 'true', 
    '--moe_aux_loss_coeff', '1e-3', 
    '--micro_batch_size', '1', 
    '--global_batch_size', '16', 
    '--packing', 'true', 
    '--recompute_granularity', 'full', 
    '--recompute_method', 'uniform', 
    '--recompute_num_layers', '1', 
    '--max_epochs', '8', 
    '--finetune', 'true', 
    '--cross_entropy_loss_fusion', 'true', 
    '--lr', '1e-4', 
    '--lr_warmup_fraction', '0.05', 
    '--min_lr', '1e-5', 
    '--save', '/models/megatron_output/GLM-4.5-Air-Debug', 
    '--eval_interval', '200', 
    '--save_interval', '500', 
    '--max_length', '4096', 
    '--num_workers', '8', 
    '--dataset_num_proc', '8', 
    '--no_save_optim', 'true', 
    '--no_save_rng', 'true', 
    '--loss_scale', 'default', 
    '--attention_backend', 'flash'
]

### self.args (MegatronSft)
MegatronTrainArguments(
    model='/models/ZhipuAI/GLM-4.5-Air', 
    model_type='glm4_5', 
    model_revision=None, 
    task_type='causal_lm', 
    torch_dtype=torch.bfloat16, 
    attn_impl=None, 
    new_special_tokens=[], 
    num_labels=None, 
    problem_type=None, 
    rope_scaling=None, 
    device_map=None, 
    max_memory={}, 
    max_model_len=None, 
    local_repo_path=None, 
    init_strategy=None, 
    template='glm4_5', 
    system=None, 
    max_length=4096, 
    truncation_strategy='delete', 
    max_pixels=None, 
    agent_template=None, 
    norm_bbox=None, 
    use_chat_template=True, 
    padding_free=True, 
    padding_side='right', 
    loss_scale='default', 
    sequence_parallel_size=2, 
    response_prefix=None, 
    template_backend='swift', 
    dataset=['/datasets/debug.jsonl'], 
    val_dataset=[], 
    split_dataset_ratio=0.1, 
    data_seed=42, 
    dataset_num_proc=8, 
    load_from_cache_file=True, 
    dataset_shuffle=True, 
    val_dataset_shuffle=False, 
    streaming=False, 
    interleave_prob=None, 
    stopping_strategy='first_exhausted', 
    shuffle_buffer_size=1000, 
    download_mode='reuse_dataset_if_exists', 
    columns={}, 
    strict=False, 
    remove_unused_columns=True, 
    model_name=None, 
    model_author=None, 
    custom_dataset_info=[], 
    quant_method=None, 
    quant_bits=None, 
    hqq_axis=None, 
    bnb_4bit_compute_dtype=torch.bfloat16, 
    bnb_4bit_quant_type='nf4', 
    bnb_4bit_use_double_quant=True, 
    bnb_4bit_quant_storage=None, 
    max_new_tokens=None, 
    temperature=None, 
    top_k=None, 
    top_p=None, 
    repetition_penalty=None, 
    num_beams=1, 
    stream=False, 
    stop_words=[], 
    logprobs=False, 
    top_logprobs=None, 
    ckpt_dir='/models/ZhipuAI/GLM-4.5-Air-mcore', 
    lora_modules=[], 
    tuner_backend='peft', 
    train_type='lora', 
    adapters=[], 
    external_plugins=[], 
    seed=42, 
    model_kwargs={}, 
    load_args=True, 
    load_data_args=False, 
    packing=True, 
    packing_length=4096, 
    lazy_tokenize=False, 
    cached_dataset=[], 
    custom_register_path=[], 
    use_hf=False, 
    hub_token=None, 
    ddp_timeout=18000000, 
    ddp_backend=None, 
    ignore_args_error=False, 
    use_swift_lora=False, 
    freeze_parameters=[], 
    freeze_parameters_regex=None, 
    freeze_parameters_ratio=0.0, 
    trainable_parameters=[], 
    trainable_parameters_regex=None, 
    adapter_load=None, 
    target_modules=['all-linear'], 
    target_regex=None, 
    modules_to_save=[], 
    lora_rank=16, 
    lora_alpha=32, 
    lora_dropout=0.05, 
    lora_bias='none', 
    lora_dtype=None, 
    use_rslora=False, 
    ref_load=None, 
    beta=0.1, 
    rpo_alpha=None, 
    reference_free=False, 
    label_smoothing=0.0, 
    f_divergence_type='reverse_kl', 
    loss_type='sigmoid', 
    padded_vocab_size=151552, 
    initialize_embedding=False, 
    mlp_padding_free=False, 
    dataloader_persistent_workers=True, 
    dataloader_prefetch_factor=10, 
    architectures='Glm4MoeForCausalLM', 
    max_epochs=8, 
    enable_dft_loss=False, 
    original_max_position_embeddings=None, 
    partial_rotary_factor=0.5, 
    use_shared_expert_gate=False, 
    micro_batch_size=1, 
    global_batch_size=16, 
    recompute_granularity='full', 
    recompute_method='uniform', 
    recompute_num_layers=1, 
    recompute_modules=['core_attn'], 
    use_cpu_initialization=False, 
    deterministic_mode=False, 
    train_iters=None, 
    log_interval=5, 
    tensorboard_dir='/models/megatron_output/GLM-4.5-Air-Debug/v0-20250827-140714/runs', 
    no_masked_softmax_fusion=False, 
    no_bias_dropout_fusion=False, 
    no_bias_swiglu_fusion=False, 
    no_rope_fusion=False, 
    no_gradient_accumulation_fusion=False, 
    cross_entropy_loss_fusion=True, 
    cross_entropy_fusion_impl='native', 
    calculate_per_token_loss=True, 
    use_flash_attn=False, 
    attention_backend='flash', 
    optimizer='adam', 
    optimizer_cpu_offload=False, 
    optimizer_offload_fraction=1.0, 
    use_precision_aware_optimizer=False, 
    main_grads_dtype='fp32', 
    main_params_dtype='fp32', 
    exp_avg_dtype='fp32', 
    exp_avg_sq_dtype='fp32', 
    dataloader_type='cyclic', 
    manual_gc=False, 
    manual_gc_interval=0, 
    lr=0.0001, 
    lr_decay_style='cosine', 
    lr_decay_iters=None, 
    lr_warmup_iters=0, 
    lr_warmup_fraction=0.05, 
    min_lr=1e-05, 
    weight_decay=0.1, 
    clip_grad=1.0, 
    adam_beta1=0.9, 
    adam_beta2=0.95, 
    adam_eps=1e-08, 
    sgd_momentum=0.9, 
    save='/models/megatron_output/GLM-4.5-Air-Debug/v0-20250827-140714', 
    save_interval=500, 
    no_save_optim=True, 
    no_save_rng=True, 
    load='/models/ZhipuAI/GLM-4.5-Air-mcore', 
    no_load_optim=False, 
    no_load_rng=False, 
    finetune=True, 
    ckpt_format='torch_dist', 
    no_initialization=True, 
    auto_detect_ckpt_format=True, 
    exit_on_missing_checkpoint=True, 
    distributed_backend='nccl', 
    local_rank=0, 
    use_distributed_optimizer=True, 
    tensor_model_parallel_size=4, 
    pipeline_model_parallel_size=2, 
    decoder_first_pipeline_num_layers=None, 
    decoder_last_pipeline_num_layers=None, 
    sequence_parallel=True, 
    context_parallel_size=2, 
    tp_comm_overlap=False, 
    overlap_grad_reduce=False, 
    overlap_param_gather=False, 
    distributed_timeout_minutes=300000, 
    num_layers=46, 
    hidden_size=4096, 
    ffn_hidden_size=10944, 
    num_attention_heads=96, 
    group_query_attention=True, 
    num_query_groups=8, 
    max_position_embeddings=131072, 
    position_embedding_type='rope', 
    rotary_base=1000000, 
    rotary_percent=1.0, 
    rotary_interleaved=False, 
    normalization='RMSNorm', 
    norm_epsilon=1e-05, 
    swiglu=True, 
    untie_embeddings_and_output_weights=True, 
    disable_bias_linear=True, 
    add_qkv_bias=True, 
    attention_dropout=0.0, 
    hidden_dropout=0.0, 
    kv_channels=128, 
    qk_layernorm=False, 
    transformer_impl='transformer_engine', 
    num_experts=128, 
    moe_layer_freq='[0]*1+[1]*45', 
    moe_ffn_hidden_size=1408, 
    moe_shared_expert_intermediate_size=1408, 
    moe_router_topk=8, 
    moe_router_pre_softmax=False, 
    moe_router_dtype='fp32', 
    moe_router_score_function='sigmoid', 
    moe_router_bias_update_rate=0.001, 
    moe_router_enable_expert_bias=True, 
    moe_router_topk_scaling_factor=1.0, 
    moe_router_load_balancing_type='aux_loss', 
    expert_model_parallel_size=4, 
    expert_tensor_parallel_size=1, 
    moe_token_dispatcher_type='alltoall', 
    moe_enable_deepep=False, 
    moe_grouped_gemm=True, 
    moe_permute_fusion=True, 
    moe_aux_loss_coeff=0.001, 
    moe_z_loss_coeff=None, 
    moe_expert_capacity_factor=None, 
    moe_shared_expert_overlap=True, 
    moe_layer_recompute=False, 
    moe_token_drop_policy='probs', 
    multi_latent_attention=False, 
    q_lora_rank=None, 
    kv_lora_rank=32, 
    qk_head_dim=128, 
    qk_pos_emb_head_dim=64, 
    fp8_format=None, 
    fp8_recipe='delayed', 
    fp8_amax_history_len=1024, 
    fp8_amax_compute_algo='max', 
    fp8_param_gather=False, 
    fp16=False, 
    bf16=True, 
    apply_query_key_layer_scaling=False, 
    attention_softmax_in_fp32=True, 
    log_params_norm=False, 
    log_throughput=False, 
    tensorboard_log_interval=1, 
    tensorboard_queue_size=50, 
    log_timers_to_tensorboard=True, 
    no_log_learning_rate_to_tensorboard=False, 
    log_validation_ppl_to_tensorboard=True, 
    log_memory_to_tensorboard=True, 
    logging_level=None, 
    wandb_project=None, 
    wandb_exp_name=None, 
    wandb_save_dir=None, 
    eval_iters=-1, 
    eval_interval=200, 
    seq_length=4096, 
    num_workers=8, 
    extra_megatron_kwargs={}, 
    add_version=True
)


### dataset_kwargs
{
    'seed': 42, 
    'num_proc': 8, 
    'load_from_cache_file': True, 
    'streaming': False, 
    'interleave_prob': None, 
    'stopping_strategy': 'first_exhausted', 
    'shuffle_buffer_size': 1000, 
    'use_hf': False, 
    'hub_token': None, 
    'download_mode': 'reuse_dataset_if_exists', 
    'columns': {}, 
    'strict': False, 
    'model_name': None, 
    'model_author': None, 
    'remove_unused_columns': True
}

> model_name 和 model_author 用来填充 self-cognition 数据集
> model_name: str | list[str] 其中第一个是中文，第二个是英文，model_author 同理


### dataset_syntax
DatasetSyntax(
    dataset='/datasets/debug.jsonl', 
    subsets=[], 
    dataset_sample=None, 
    use_hf=None
)

### dataset_meta
DatasetMeta(
    ms_dataset_id=None, 
    hf_dataset_id=None, 
    dataset_path=None, 
    dataset_name=None, 
    ms_revision=None, 
    hf_revision=None, 
    subsets=[
        SubsetDataset(
            name='default', 
            subset='default', 
            split=None, 
            preprocess_func=None, 
            is_weak_subset=False
        )
    ], 
    split=['train'], 
    preprocess_func=<swift.llm.dataset.preprocessor.core.AutoPreprocessor object at 0x7fe76c0e0810>, 
    load_function=<function DatasetLoader.load at 0x7fe6d13d39c0>, 
    tags=[], 
    help=None, 
    huge_dataset=False
)

### dataset
Dataset({
    features: [
        'messages', 'tools', 'input', 'content', 'reasoning_content', 'repo_name', 'prompt_tokens_len', 'content_tokens_len', 'reasoning_content_tokens_len', 'score'
    ],
    num_rows: 100
})


### features
{
    'messages': [
        {
            'role': Value(dtype='string', id=None), 
            'content': Value(dtype='string', id=None)
        }
    ], 
    'tools': Value(dtype='string', id=None), 
    'input': Value(dtype='string', id=None), 
    'content': Value(dtype='string', id=None), 
    'reasoning_content': Value(dtype='string', id=None), 
    'repo_name': Value(dtype='string', id=None), 
    'prompt_tokens_len': Value(dtype='int64', id=None), 
    'content_tokens_len': Value(dtype='int64', id=None), 
    'reasoning_content_tokens_len': Value(dtype='int64', id=None), 
    'score': Value(dtype='int64', id=None)
}


### MessagesPreprocessor.columns
{
    'images': 'images', 
    'image': 'images', 
    'audios': 'audios', 
    'audio': 'audios', 
    'videos': 'videos', 
    'video': 'videos', 
    'messages': 'messages', 
    'conversation': 'messages', 
    'conversations': 'messages', 
    'system': 'system',
    'system_prompt': 'system'
}


### dataset_mapped
Dataset({
    features: ['messages', 'tools', 'input', 'content', 'reasoning_content', 'repo_name', 'prompt_tokens_len', 'content_tokens_len', 'reasoning_content_tokens_len', 'score'],
    num_rows: 98
})



### 处理后的 dataset
Dataset({
    features: ['messages', 'tools'],
    num_rows: 98
})


### agent_template 
`<swift.plugin.agent_template.glm4.GLM4_5AgentTemplate object at 0x7f4a07a0a390>`


### template_meta
GLM4_5TemplateMeta(
    template_type='glm4_5', 
    prefix=['[gMASK]<sop>'], 
    prompt=['<|user|>\n{{QUERY}}<|assistant|>\n'], 
    chat_sep=[], 
    suffix=['<|user|>'], 
    template_cls=<class 'swift.llm.template.template.utils.ThinkingTemplate'>, 
    system_prefix=['[gMASK]<sop><|system|>\n{{SYSTEM}}'], 
    default_system=None, 
    response_prefix='', 
    auto_add_bos=True, 
    stop_words=[
        '<|endoftext|>', 
        '<|user|>', 
        '<|observation|>'
    ], 
    agent_template='glm4_5'
)

### res_context_list
[
    '[gMASK]<sop>', 
    '<|user|>\n这里是问题<|assistant|>\n', 
    '<think></think>\n这里是回答', 
    '<|user|>'
]

### loss_scale_list
[0.0, 0.0, 1.0, 1.0]

### answer_len
2


### _GLOBAL_ARGS = args
Namespace(
    num_layers=46, 
    encoder_num_layers=46, 
    decoder_num_layers=None, 
    hidden_size=4096, 
    ffn_hidden_size=10944, 
    num_attention_heads=96, 
    attention_backend=<AttnBackend.flash: 1>, 
    kv_channels=128, 
    group_query_attention=True, 
    num_query_groups=8, 
    max_position_embeddings=131072, 
    position_embedding_type='rope', 
    relative_attention_num_buckets=32, 
    relative_attention_max_distance=128, 
    use_rotary_position_embeddings=False, 
    rotary_base=1000000, 
    rotary_percent=1.0, 
    rotary_interleaved=False, 
    rotary_seq_len_interpolation_factor=None, 
    use_rope_scaling=False, 
    rope_scaling_factor=8.0, 
    no_rope_freq=None, 
    add_position_embedding=True, 
    mrope_section=None, 
    make_vocab_size_divisible_by=128, 
    normalization='RMSNorm', 
    norm_epsilon=1e-05, 
    apply_layernorm_1p=False, 
    apply_residual_connection_post_layernorm=False, 
    openai_gelu=False, 
    squared_relu=False, 
    swiglu=True, 
    onnx_safe=None, 
    bert_binary_head=True, 
    untie_embeddings_and_output_weights=True, 
    multi_latent_attention=False, 
    mtp_num_layers=None, 
    mtp_loss_scaling_factor=0.1, 
    attention_dropout=0.0, 
    hidden_dropout=0.0, 
    weight_decay=0.1, 
    start_weight_decay=0.1, 
    end_weight_decay=0.1, 
    weight_decay_incr_style='constant', 
    clip_grad=1.0, 
    adam_beta1=0.9, 
    adam_beta2=0.95, 
    adam_eps=1e-08, 
    sgd_momentum=0.9, 
    micro_batch_size=1, 
    global_batch_size=16, 
    rampup_batch_size=None, 
    decrease_batch_size_if_needed=False, 
    recompute_granularity='full', 
    check_for_nan_in_loss_and_grad=True, 
    check_for_spiky_loss=False, 
    check_for_large_grads=False, 
    distribute_saved_activations=False, 
    recompute_method='uniform', 
    recompute_num_layers=1, 
    recompute_modules=['core_attn'], 
    clone_scatter_output_in_embedding=True, 
    profile=False, 
    profile_step_start=10, 
    profile_step_end=12, 
    iterations_to_skip=[], 
    result_rejected_tracker_filename=None, 
    enable_gloo_process_groups=True, 
    use_pytorch_profiler=False, 
    profile_ranks=[0], 
    record_memory_history=False, 
    memory_snapshot_path='snapshot.pickle', 
    tp_comm_overlap=False, 
    tp_comm_overlap_cfg=None, 
    tp_comm_overlap_ag=True, 
    tp_comm_overlap_rs=True, 
    tp_comm_overlap_rs_dgrad=False, 
    tp_comm_bulk_dgrad=True, 
    tp_comm_bulk_wgrad=True, 
    tp_comm_bootstrap_backend='nccl', 
    use_cpu_initialization=None, 
    empty_unused_memory_level=0, 
    deterministic_mode=False, 
    check_weight_hash_across_dp_replicas_interval=None, 
    calculate_per_token_loss=True, 
    train_sync_interval=None, 
    train_iters=None, 
    train_samples=None, 
    log_interval=5, 
    exit_interval=None, 
    exit_duration_in_mins=None, 
    exit_signal_handler=False, 
    tensorboard_dir='/models/megatron_output/GLM-4.5-Air-Debug/v9-20250901-152939/runs', 
    masked_softmax_fusion=True, 
    bias_gelu_fusion=False, 
    bias_swiglu_fusion=True, 
    bias_dropout_fusion=True, 
    apply_rope_fusion=True, 
    cross_entropy_loss_fusion=True, 
    cross_entropy_fusion_impl='native', 
    use_flash_attn=False, 
    add_bias_linear=False, 
    add_qkv_bias=True, 
    optimizer='adam', 
    optimizer_cpu_offload=False, 
    optimizer_offload_fraction=1.0, 
    use_torch_optimizer_for_cpu_offload=False, 
    overlap_cpu_optimizer_d2h_h2d=False, 
    pin_cpu_grads=True, 
    pin_cpu_params=True, 
    dataloader_type='cyclic', 
    async_tensor_model_parallel_allreduce=True, 
    no_persist_layer_norm=False, 
    sequence_parallel=True, 
    gradient_accumulation_fusion=True, 
    deprecated_use_mcore_models=False, 
    use_legacy_models=False, 
    manual_gc=False, 
    manual_gc_interval=0, 
    manual_gc_eval=True, 
    tp_comm_split_ag=True, 
    tp_comm_split_rs=True, 
    pipeline_model_parallel_comm_backend=None, 
    high_priority_stream_groups=[], 
    seed=42, 
    data_parallel_random_init=False, 
    init_method_std=0.02, 
    init_method_xavier_uniform=False, 
    lr=0.0001, 
    lr_decay_style='cosine', 
    lr_wsd_decay_style='exponential', 
    lr_decay_iters=None, 
    lr_decay_samples=None, 
    lr_wsd_decay_samples=None, 
    lr_wsd_decay_iters=None, 
    lr_warmup_fraction=0.05, 
    lr_warmup_iters=0, 
    lr_warmup_samples=0, 
    lr_warmup_init=0.0, 
    min_lr=1e-05, 
    override_opt_param_scheduler=False, 
    use_checkpoint_opt_param_scheduler=False, 
    decoupled_lr=None, 
    decoupled_min_lr=None, 
    save='/models/megatron_output/GLM-4.5-Air-Debug/v9-20250901-152939', 
    save_interval=500, 
    no_save_optim=True, 
    no_save_rng=True, 
    load='/models/ZhipuAI/GLM-4.5-Air-mcore', 
    no_load_optim=None, 
    no_load_rng=None, 
    non_persistent_save_interval=None, 
    non_persistent_ckpt_type=None, 
    non_persistent_global_ckpt_dir=None, 
    non_persistent_local_ckpt_dir=None, 
    non_persistent_local_ckpt_algo='fully_parallel', 
    finetune=True, 
    pretrained_checkpoint=None, 
    ckpt_step=None, 
    perform_initialization=False, 
    use_checkpoint_args=False, 
    use_mp_args_from_checkpoint_args=False, 
    use_tokenizer_model_from_checkpoint_args=True, 
    exit_on_missing_checkpoint=True, 
    use_dist_ckpt_deprecated=False, 
    use_persistent_ckpt_worker=False, 
    auto_detect_ckpt_format=True, 
    dist_ckpt_format_deprecated=None, 
    ckpt_format='torch_dist', 
    ckpt_convert_format=None, 
    ckpt_convert_save=None, 
    ckpt_convert_update_legacy_dist_opt_format=False, 
    ckpt_fully_parallel_save_deprecated=False, 
    ckpt_fully_parallel_save=True, 
    async_save=None, 
    ckpt_fully_parallel_load=False, 
    ckpt_assume_constant_structure=False, 
    dist_ckpt_strictness='assume_ok_unexpected', 
    load_model_opt_format=False, 
    fp16=False, 
    bf16=True, 
    grad_reduce_in_bf16=False, 
    loss_scale=None, 
    initial_loss_scale=4294967296, 
    min_loss_scale=1.0, 
    loss_scale_window=1000, 
    hysteresis=2, 
    fp32_residual_connection=False, 
    apply_query_key_layer_scaling=False, 
    attention_softmax_in_fp32=True, 
    accumulate_allreduce_grads_in_fp32=True, 
    fp16_lm_cross_entropy=False, 
    disable_bf16_reduced_precision_matmul=False, 
    reuse_grad_buf_for_mxfp8_param_ag=False, 
    tensor_model_parallel_size=4, 
    encoder_tensor_model_parallel_size=0, 
    pipeline_model_parallel_size=2, 
    encoder_pipeline_model_parallel_size=0, 
    pipeline_model_parallel_split_rank=None, 
    decoder_first_pipeline_num_layers=None, 
    decoder_last_pipeline_num_layers=None, 
    pipeline_model_parallel_layout=None, 
    num_layers_per_virtual_pipeline_stage=None, 
    num_virtual_stages_per_pipeline_rank=None, 
    microbatch_group_size_per_vp_stage=None, 
    overlap_p2p_comm=False, 
    overlap_p2p_comm_warmup_flush=False, 
    distributed_backend='nccl', 
    distributed_timeout_minutes=300000, 
    overlap_grad_reduce=False, 
    defer_embedding_wgrad_compute=False, 
    wgrad_deferral_limit=0, 
    align_grad_reduce=True, 
    ddp_num_buckets=None, 
    ddp_bucket_size=None, 
    ddp_pad_buckets_for_high_nccl_busbw=False, 
    ddp_average_in_collective=False, 
    overlap_param_gather=False, 
    overlap_param_gather_with_optimizer_step=False, 
    align_param_gather=False, 
    scatter_gather_tensors_in_pipeline=True, 
    use_ring_exchange_p2p=False, 
    local_rank=0, 
    lazy_mpu_init=None, 
    account_for_embedding_in_pipeline_split=False, 
    account_for_loss_in_pipeline_split=False, 
    use_distributed_optimizer=True, 
    nccl_ub=False, 
    use_sharp=False, 
    use_custom_fsdp=False, 
    init_model_with_meta_device=False, 
    data_parallel_sharding_strategy='no_shard', 
    gradient_reduce_div_fusion=True, 
    fsdp_double_buffer=False, 
    suggested_communication_unit_size=None, 
    keep_fp8_transpose_cache_when_using_custom_fsdp=False, 
    num_distributed_optimizer_instances=1, 
    use_torch_fsdp2=False, 
    torch_fsdp2_reshard_after_forward=True, 
    context_parallel_size=2, 
    cp_comm_type=['p2p'], 
    hierarchical_context_parallel_sizes=None, 
    nccl_communicator_config_path=None, 
    use_tp_pp_dp_mapping=False, 
    replication=False, 
    replication_jump=None, 
    replication_factor=2, 
    eval_iters=-1, 
    eval_interval=200, 
    test_mode=False, 
    skip_train=False, 
    data_path=None, 
    split=None, 
    train_data_path=None, 
    valid_data_path=None, 
    test_data_path=None, 
    data_args_path=None, 
    per_split_data_args_path=None, 
    data_cache_path=None, 
    mmap_bin_files=True, 
    mock_data=False, 
    seq_length=4096, 
    encoder_seq_length=4096, 
    decoder_seq_length=None, 
    retriever_seq_length=256, 
    sample_rate=1.0, 
    mask_prob=0.15, 
    short_seq_prob=0.1, 
    num_workers=8, 
    reset_position_ids=False, 
    reset_attention_mask=False, 
    eod_mask_loss=False, 
    create_attention_mask_in_dataloader=True, 
    num_dataset_builder_threads=1, 
    object_storage_cache_path=None, 
    mid_level_dataset_surplus=0.005, 
    vocab_size=None, 
    vocab_file=None, 
    merge_file=None, 
    vocab_extra_ids=0, 
    tokenizer_type=None, 
    tokenizer_model=None, 
    tiktoken_pattern=None, 
    tiktoken_num_special_tokens=1000, 
    tiktoken_special_tokens=None, 
    adlr_autoresume=False, 
    adlr_autoresume_interval=1000, 
    ict_head_size=None, 
    biencoder_projection_dim=0, 
    biencoder_shared_query_context_model=False, 
    ict_load=None, 
    bert_load=None, 
    titles_data_path=None, 
    query_in_block_prob=0.1, 
    use_one_sent_docs=False, 
    evidence_data_path=None, 
    retriever_report_topk_accuracies=[], 
    retriever_score_scaling=False, 
    block_data_path=None, 
    embedding_path=None, 
    indexer_batch_size=128, 
    indexer_log_interval=1000, 
    num_classes=1000, 
    img_h=224, 
    img_w=224, 
    num_channels=3, 
    patch_dim=16, 
    classes_fraction=1.0, 
    data_per_class_fraction=1.0, 
    data_sharding=True, 
    head_lr_mult=1.0, 
    vision_pretraining=False, 
    vision_pretraining_type='classify', 
    vision_backbone_type='vit', 
    swin_backbone_type='tiny', 
    mask_type='random', 
    mask_factor=1.0, 
    iter_per_epoch=1250, 
    dino_local_img_size=96, 
    dino_local_crops_number=10, 
    dino_head_hidden_size=2048, 
    dino_bottleneck_size=256, 
    dino_freeze_last_layer=1, 
    dino_norm_last_layer=False, 
    dino_warmup_teacher_temp=0.04, 
    dino_teacher_temp=0.07, 
    dino_warmup_teacher_temp_epochs=30, 
    qk_layernorm=False, 
    qk_l2_norm=False, 
    expert_model_parallel_size=4, 
    expert_tensor_parallel_size=1, 
    num_experts=128, 
    moe_layer_freq=[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
    moe_ffn_hidden_size=1408, 
    moe_shared_expert_intermediate_size=1408, 
    moe_shared_expert_overlap=True, 
    moe_grouped_gemm=True, 
    moe_use_legacy_grouped_gemm=False, 
    moe_layer_recompute=False, 
    moe_extended_tp=False, 
    moe_use_upcycling=False, 
    moe_router_load_balancing_type='aux_loss', 
    moe_router_dtype='fp32', 
    moe_router_score_function='sigmoid', 
    moe_router_topk=8, 
    moe_router_pre_softmax=False, 
    moe_router_num_groups=None, 
    moe_router_group_topk=None, 
    moe_router_topk_scaling_factor=1.0, 
    moe_router_enable_expert_bias=True, 
    moe_router_bias_update_rate=0.001, 
    moe_router_force_load_balancing=False, 
    moe_router_padding_for_fp8=False, 
    moe_aux_loss_coeff=0.001, 
    moe_z_loss_coeff=None, 
    moe_input_jitter_eps=None, 
    moe_per_layer_logging=False, 
    moe_token_dispatcher_type='alltoall', 
    moe_enable_deepep=False, 
    moe_deepep_num_sms=20, 
    moe_permute_fusion=True, 
    moe_expert_capacity_factor=None, 
    moe_pad_expert_input_to_capacity=False, 
    moe_token_drop_policy='probs', 
    moe_apply_probs_on_input=False, 
    delay_wgrad_compute=False, 
    moe_upcycling_granularity=1, 
    q_lora_rank=None, 
    kv_lora_rank=32, 
    qk_head_dim=128, 
    qk_pos_emb_head_dim=64, 
    v_head_dim=128, 
    rotary_scaling_factor=1.0, 
    mscale=1.0, 
    mscale_all_dim=1.0, 
    heterogeneous_layers_config_path=None, 
    heterogeneous_layers_config_encoded_json=None, 
    log_params_norm=False, 
    log_num_zeros_in_grad=False, 
    log_throughput=False, 
    log_progress=False, 
    timing_log_level=0, 
    log_energy=False, 
    barrier_with_L1_time=True, 
    timing_log_option='minmax', 
    tensorboard_log_interval=1, 
    tensorboard_queue_size=50, 
    log_timers_to_tensorboard=True, 
    log_loss_scale_to_tensorboard=True, 
    log_validation_ppl_to_tensorboard=True, 
    log_memory_to_tensorboard=True, 
    log_world_size_to_tensorboard=False, 
    wandb_project='', 
    wandb_exp_name='', 
    wandb_save_dir='', 
    logging_level=None, 
    log_straggler=False, 
    disable_straggler_on_startup=False, 
    straggler_ctrlr_port=65535, 
    straggler_minmax_count=1, 
    run_workload_inspector_server=False, 
    inference_batch_times_seqlen_threshold=-1, 
    max_tokens_to_oom=12000, 
    output_bert_embeddings=False, 
    bert_embedder_type='megatron', 
    flash_decode=False, 
    enable_cuda_graph=False, 
    cuda_graph_warmup_steps=3, 
    external_cuda_graph=False, 
    cuda_graph_scope='full', 
    inference_max_batch_size=8, 
    inference_max_seq_length=2560, 
    inference_dynamic_batching=False, 
    inference_dynamic_batching_buffer_size_gb=40.0, 
    inference_dynamic_batching_chunk_size=256, 
    inference_dynamic_batching_buffer_guaranteed_fraction=0.2, 
    inference_dynamic_batching_buffer_overflow_factor=None, 
    inference_dynamic_batching_max_requests_override=None, 
    inference_dynamic_batching_max_tokens_override=None, 
    symmetric_ar_type=None, 
    nccl_all_reduce_for_prefill=False, 
    mlp_chunks_for_prefill=1, 
    fp8=None, 
    fp8_recipe='delayed', 
    fp8_margin=0, 
    fp8_interval=1, 
    fp8_amax_history_len=1024, 
    fp8_amax_compute_algo='max', 
    fp8_wgrad=True, 
    transformer_impl='transformer_engine', 
    fp8_param_gather=False, 
    first_last_layers_bf16=False, 
    num_layers_at_start_in_bf16=1, 
    num_layers_at_end_in_bf16=1, 
    te_rng_tracker=False, 
    inference_rng_tracker=False, 
    retro_project_dir=None, 
    retro_add_retriever=False, 
    retro_cyclic_train_iters=None, 
    retro_encoder_layers=2, 
    retro_encoder_hidden_dropout=0.1, 
    retro_encoder_attention_dropout=0.1, 
    retro_num_neighbors=2, 
    retro_num_retrieved_chunks=2, 
    retro_attention_gate=1, 
    retro_verify_neighbor_count=True, 
    enable_experimental=False, 
    spec=None, 
    hybrid_attention_ratio=0.0, 
    hybrid_mlp_ratio=0.0, 
    hybrid_override_pattern=None, 
    mamba_state_dim=128, 
    mamba_head_dim=64, 
    mamba_num_groups=8, 
    mamba_num_heads=None, 
    is_hybrid_model=False, 
    disable_mamba_mem_eff_path=False, 
    yaml_cfg=None, 
    use_precision_aware_optimizer=False, 
    main_grads_dtype=torch.float32, 
    main_params_dtype=torch.float32, 
    exp_avg_dtype=torch.float32, 
    exp_avg_sq_dtype=torch.float32, 
    enable_one_logger=True, 
    one_logger_project='megatron-lm', 
    one_logger_run_name=None, 
    one_logger_async=False, 
    app_tag_run_name=None, 
    app_tag_run_version='0.0.0', 
    inprocess_restart=False, 
    inprocess_max_iterations=None, 
    inprocess_monitor_thread_interval=1.0, 
    inprocess_monitor_process_interval=1.0, 
    inprocess_progress_watchdog_interval=1.0, 
    inprocess_heartbeat_interval=30, 
    inprocess_soft_timeout=60, 
    inprocess_hard_timeout=90, 
    inprocess_heartbeat_timeout=60, 
    inprocess_barrier_timeout=120, 
    inprocess_completion_timeout=120, 
    inprocess_last_call_wait=1, 
    inprocess_termination_grace_time=1, 
    inprocess_granularity='node', 
    inprocess_active_world_size=16, 
    inprocess_empty_cuda_cache=False, 
    enable_ft_package=False, 
    calc_ft_timeouts=False, 
    config_logger_dir='', 
    error_injection_rate=0, 
    error_injection_type='transient_error', 
    rerun_mode='disabled', 
    enable_msc=True, 
    kitchen_config_file=None, 
    kitchen_recipe_number=None, 
    sft=False, 
    sft_tokenizer_prompt_format='nemotron-h-aligned', 
    rank=0, 
    world_size=16, 
    use_dist_ckpt=True, 
    transformer_pipeline_model_parallel_size=2, 
    data_parallel_size=1, 
    model='/models/ZhipuAI/GLM-4.5-Air', 
    model_type='glm4_5', 
    model_revision=None, 
    task_type='causal_lm', 
    torch_dtype=torch.bfloat16, 
    attn_impl=None, 
    new_special_tokens=[], 
    num_labels=None, 
    problem_type=None, 
    rope_scaling=None, 
    device_map=None, 
    max_memory={}, 
    max_model_len=None, 
    local_repo_path=None, 
    init_strategy=None, 
    template='glm4_5', 
    system=None, 
    max_length=4096, 
    truncation_strategy='delete', 
    max_pixels=None, 
    agent_template=None, 
    norm_bbox=None, 
    use_chat_template=True, 
    padding_free=True, 
    padding_side='right', 
    sequence_parallel_size=2, 
    response_prefix=None, 
    template_backend='swift', 
    dataset=['/datasets/debug.jsonl'], 
    val_dataset=[], 
    split_dataset_ratio=0.1, 
    data_seed=42, 
    dataset_num_proc=8, 
    load_from_cache_file=True, 
    dataset_shuffle=True, 
    val_dataset_shuffle=False, 
    streaming=False, 
    interleave_prob=None, 
    stopping_strategy='first_exhausted', 
    shuffle_buffer_size=1000, 
    download_mode='reuse_dataset_if_exists', 
    columns={}, 
    strict=False, 
    remove_unused_columns=True, 
    model_name=None, 
    model_author=None, 
    custom_dataset_info=[], 
    quant_method=None, 
    quant_bits=None, 
    hqq_axis=None, 
    bnb_4bit_compute_dtype=torch.bfloat16, 
    bnb_4bit_quant_type='nf4', 
    bnb_4bit_use_double_quant=True, 
    bnb_4bit_quant_storage=None, 
    max_new_tokens=None, 
    temperature=None, 
    top_k=None, 
    top_p=None, 
    repetition_penalty=None, 
    num_beams=1, 
    stream=False, 
    stop_words=[], 
    logprobs=False, 
    top_logprobs=None, 
    ckpt_dir='/models/ZhipuAI/GLM-4.5-Air-mcore', 
    lora_modules=[], 
    tuner_backend='peft', 
    train_type='lora', 
    adapters=[], 
    external_plugins=[], 
    model_kwargs={}, 
    load_args=True, 
    load_data_args=False, 
    packing=True, 
    packing_length=4096, 
    lazy_tokenize=False, 
    cached_dataset=[], 
    custom_register_path=[], 
    use_hf=False, 
    hub_token=None, 
    ddp_timeout=18000000, 
    ddp_backend=None, 
    ignore_args_error=False, 
    use_swift_lora=False, 
    freeze_parameters=[], 
    freeze_parameters_regex=None, 
    freeze_parameters_ratio=0.0, 
    trainable_parameters=[], 
    trainable_parameters_regex=None, 
    adapter_load=None, 
    target_modules=['all-linear'], 
    target_regex=None, 
    modules_to_save=[], 
    lora_rank=16, 
    lora_alpha=32, 
    lora_dropout=0.05, 
    lora_bias='none', 
    lora_dtype=None, 
    use_rslora=False, 
    ref_load=None, 
    beta=0.1, 
    rpo_alpha=None, 
    reference_free=False, 
    label_smoothing=0.0, 
    f_divergence_type='reverse_kl', 
    loss_type='sigmoid', 
    padded_vocab_size=151552, 
    initialize_embedding=False, 
    mlp_padding_free=False, 
    dataloader_persistent_workers=True, 
    dataloader_prefetch_factor=10, 
    architectures='Glm4MoeForCausalLM', 
    max_epochs=8, 
    enable_dft_loss=False, 
    original_max_position_embeddings=None, 
    partial_rotary_factor=0.5, 
    use_shared_expert_gate=False, 
    add_version=True, 
    virtual_pipeline_model_parallel_size=None, 
    params_dtype=torch.bfloat16, 
    consumed_train_samples=0, 
    skipped_train_samples=0, 
    consumed_valid_samples=0, 
    variable_seq_lengths=False
)


### TransformerConfig
TransformerConfig(
    tensor_model_parallel_size=4, 
    pipeline_model_parallel_comm_backend=None, 
    pipeline_model_parallel_size=2, 
    virtual_pipeline_model_parallel_size=None, 
    sequence_parallel=True, 
    context_parallel_size=2, 
    hierarchical_context_parallel_sizes=None, 
    expert_model_parallel_size=4, 
    expert_tensor_parallel_size=1, 
    moe_extended_tp=False, 
    perform_initialization=False, 
    use_cpu_initialization=None, 
    fp16=False, 
    bf16=True, 
    params_dtype=torch.bfloat16, 
    timers=None, 
    finalize_model_grads_func=None, 
    grad_scale_func=None, 
    no_sync_func=None, 
    grad_sync_func=None, 
    param_sync_func=None, 
    deterministic_mode=False, 
    enable_autocast=False, 
    autocast_dtype=torch.bfloat16, 
    num_microbatches_with_partial_activation_checkpoints=None, 
    gradient_accumulation_fusion=True, 
    async_tensor_model_parallel_allreduce=True, 
    use_te_rng_tracker=False, 
    tp_comm_overlap=False, 
    tp_comm_bulk_wgrad=True, 
    tp_comm_bulk_dgrad=True, 
    tp_comm_overlap_ag=True, 
    tp_comm_overlap_rs=True, 
    tp_comm_overlap_rs_dgrad=False, 
    tp_comm_split_ag=True, 
    tp_comm_atomic_ag=False, 
    tp_comm_split_rs=True, 
    tp_comm_atomic_rs=False, 
    cross_entropy_loss_fusion=True, 
    cross_entropy_fusion_impl='native', 
    tp_comm_overlap_disable_qkv=False, 
    tp_comm_overlap_disable_fc1=False, 
    tp_comm_bootstrap_backend='nccl', 
    pipeline_dtype=torch.bfloat16, 
    variable_seq_lengths=True, 
    overlap_p2p_comm=False, 
    batch_p2p_comm=True, 
    batch_p2p_sync=True, 
    use_ring_exchange_p2p=False, 
    deallocate_pipeline_outputs=True, 
    defer_embedding_wgrad_compute=False, 
    wgrad_deferral_limit=0, 
    pipeline_model_parallel_split_rank=None, 
    overlap_p2p_comm_warmup_flush=False, 
    microbatch_group_size_per_vp_stage=2, 
    delay_wgrad_compute=False, 
    cpu_offloading=False, 
    cpu_offloading_num_layers=0, 
    _cpu_offloading_context=None, 
    cpu_offloading_activations=True, 
    cpu_offloading_weights=True, 
    barrier_with_L1_time=True, 
    num_layers=46, 
    mtp_num_layers=None, 
    mtp_loss_scaling_factor=0.1, 
    num_layers_in_first_pipeline_stage=None, 
    num_layers_in_last_pipeline_stage=None, 
    pipeline_model_parallel_layout=None, 
    account_for_embedding_in_pipeline_split=False, 
    account_for_loss_in_pipeline_split=False, 
    hidden_size=4096, 
    num_attention_heads=96, 
    attention_backend=<AttnBackend.flash: 1>, 
    softmax_scale=None, 
    num_query_groups=8, 
    ffn_hidden_size=10944, 
    kv_channels=128, 
    hidden_dropout=0.0, 
    attention_dropout=0.0, 
    fp32_residual_connection=False, 
    apply_residual_connection_post_layernorm=False, 
    layernorm_epsilon=1e-05, 
    layernorm_zero_centered_gamma=False, 
    add_bias_linear=False, 
    add_qkv_bias=True, 
    gated_linear_unit=True, 
    activation_func=<function silu at 0x7f27abcc5760>, 
    activation_func_fp8_input_store=False, 
    num_moe_experts=128, 
    rotary_interleaved=False, 
    window_size=None, 
    normalization='RMSNorm', 
    qk_layernorm=False, 
    test_mode=False, 
    calculate_per_token_loss=True, 
    multi_latent_attention=False, 
    no_rope_freq=None, 
    moe_deepep_num_sms=20, 
    init_method=functools.partial(<function normal_ at 0x7f27abb3b740>, 
    mean=0.0, 
    std=0.02), 
    output_layer_init_method=functools.partial(<function normal_ at 0x7f27abb3b740>, 
    mean=0.0, 
    std=0.002085144140570748), 
    init_method_std=0.02, 
    init_model_with_meta_device=False, 
    apply_query_key_layer_scaling=False, 
    attention_softmax_in_fp32=True, 
    disable_bf16_reduced_precision_matmul=False, 
    bias_activation_fusion=True, 
    masked_softmax_fusion=True, 
    persist_layer_norm=True, 
    memory_efficient_layer_norm=False, 
    bias_dropout_fusion=True, 
    apply_rope_fusion=True, 
    recompute_granularity='full', 
    recompute_method='uniform', 
    recompute_num_layers=1, 
    distribute_saved_activations=False, 
    recompute_modules=['core_attn'], 
    fp8=None, 
    fp8_recipe='delayed', 
    fp8_param=False, 
    fp8_margin=0, 
    fp8_interval=1, 
    fp8_amax_history_len=1024, 
    fp8_amax_compute_algo='max', 
    fp8_wgrad=True, 
    fp8_dot_product_attention=False, 
    fp8_multi_head_attention=False, 
    tp_only_amax_red=False, 
    first_last_layers_bf16=False, 
    num_layers_at_start_in_bf16=1, 
    num_layers_at_end_in_bf16=1, 
    use_kitchen=False, 
    moe_shared_expert_intermediate_size=1408, 
    moe_shared_expert_overlap=True, 
    moe_layer_freq=[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 
    moe_ffn_hidden_size=1408, 
    moe_router_load_balancing_type='aux_loss', 
    moe_router_topk=8, 
    moe_router_topk_limited_devices=None, 
    moe_router_padding_for_fp8=False, 
    moe_router_num_groups=None, 
    moe_router_group_topk=None, 
    moe_router_pre_softmax=False, 
    moe_router_topk_scaling_factor=1.0, 
    moe_router_score_function='sigmoid', 
    moe_router_dtype='fp32', 
    moe_router_enable_expert_bias=True, 
    moe_router_bias_update_rate=0.001, 
    moe_router_force_load_balancing=False, 
    moe_grouped_gemm=True, 
    moe_use_legacy_grouped_gemm=False, 
    moe_aux_loss_coeff=0.001, 
    moe_z_loss_coeff=None, 
    moe_input_jitter_eps=None, 
    moe_token_dropping=False, 
    moe_token_dispatcher_type='alltoall', 
    moe_enable_deepep=False, 
    moe_per_layer_logging=False, 
    moe_expert_capacity_factor=None, 
    moe_pad_expert_input_to_capacity=False, 
    moe_token_drop_policy='probs', 
    moe_layer_recompute=False, 
    moe_permute_fusion=True, 
    moe_apply_probs_on_input=False, 
    cp_comm_type='p2p', 
    enable_cuda_graph=False, 
    cuda_graph_use_single_mempool=False, 
    cuda_graph_retain_backward_graph=False, 
    cuda_graph_warmup_steps=3, 
    external_cuda_graph=False, 
    cuda_graph_scope='full', 
    clone_scatter_output_in_embedding=True, 
    disable_parameter_transpose_cache=False, 
    config_logger_dir='', 
    flash_decode=False, 
    inference_rng_tracker=False, 
    symmetric_ar_type=None, 
    mrope_section=None, 
    is_hybrid_model=False, 
    mamba_state_dim=128, 
    mamba_head_dim=64, 
    mamba_num_groups=8, 
    mamba_num_heads=None, 
    use_mamba_mem_eff_path=True, 
    mlp_chunks_for_prefill=1, 
    heterogeneous_block_specs=False, 
    hetereogenous_dist_checkpoint=False, 
    quant_recipe=None
)

### moe_module_spec
ModuleSpec(
    module=<class 'megatron.core.transformer.moe.moe_layer.MoELayer'>, 
    params={}, 
    submodules=MoESubmodules(
        experts=ModuleSpec(
            module=<class 'megatron.core.transformer.moe.experts.TEGroupedMLP'>, 
            params={}, 
            submodules=MLPSubmodules(
                linear_fc1=<class 'megatron.core.extensions.transformer_engine.TEColumnParallelGroupedLinear'>, 
                linear_fc2=<class 'megatron.core.extensions.transformer_engine.TERowParallelGroupedLinear'>
            )
        ), 
        shared_experts=ModuleSpec(
            module=<class 'megatron.core.transformer.moe.shared_experts.SharedExpertMLP'>, 
            params={'gate': False}, 
            submodules=MLPSubmodules(
                linear_fc1=<class 'megatron.core.extensions.transformer_engine.TEColumnParallelLinear'>, 
                linear_fc2=<class 'megatron.core.extensions.transformer_engine.TERowParallelLinear'>
            )
        )
    )
)

### local_layer_specs
local_layer_specs[0]:
ModuleSpec(
    module=<class 'megatron.core.transformer.transformer_layer.TransformerLayer'>, 
    params={}, 
    submodules=TransformerLayerSubmodules(
        input_layernorm=<class 'megatron.core.transformer.identity_op.IdentityOp'>, 
        self_attention=ModuleSpec(
            module=<class 'megatron.core.transformer.attention.SelfAttention'>, 
            params={'attn_mask_type': <AttnMaskType.causal: 2>}, 
            submodules=SelfAttentionSubmodules(
                linear_qkv=<class 'megatron.core.extensions.transformer_engine.TELayerNormColumnParallelLinear'>, 
                core_attention=<class 'megatron.core.extensions.transformer_engine.TEDotProductAttention'>, 
                linear_proj=<class 'megatron.core.extensions.transformer_engine.TERowParallelLinear'>, 
                q_layernorm=<class 'megatron.core.transformer.identity_op.IdentityOp'>, 
                k_layernorm=<class 'megatron.core.transformer.identity_op.IdentityOp'>
            )
        ), 
        self_attn_bda=<function get_bias_dropout_add at 0x7fd44de8dda0>, 
        pre_cross_attn_layernorm=<class 'megatron.core.transformer.identity_op.IdentityOp'>, 
        cross_attention=<class 'megatron.core.transformer.identity_op.IdentityOp'>, 
        cross_attn_bda=<class 'megatron.core.transformer.identity_op.IdentityFuncOp'>, 
        pre_mlp_layernorm=<class 'megatron.core.transformer.identity_op.IdentityOp'>, 
        mlp=ModuleSpec(
            module=<class 'megatron.core.transformer.mlp.MLP'>, 
            params={}, 
            submodules=MLPSubmodules(
                linear_fc1=<class 'megatron.core.extensions.transformer_engine.TELayerNormColumnParallelLinear'>, 
                linear_fc2=<class 'megatron.core.extensions.transformer_engine.TERowParallelLinear'>
            )
        ), 
        mlp_bda=<function get_bias_dropout_add at 0x7fd44de8dda0>, 
        sharded_state_dict_keys_map={
            'mlp.0.weight': 'mlp.linear_fc1.layer_norm_weight', 
            'mlp.0.bias': 'mlp.linear_fc1.layer_norm_bias', 
            'mlp.1.basic_ops.0.weight': 'mlp.linear_fc1.weight', 
            'mlp.1.basic_ops.1.bias': 'mlp.linear_fc1.bias', 
            'mlp.3.basic_ops.0.weight': 'mlp.linear_fc2.weight', 
            'mlp.3.basic_ops.1.bias': 'mlp.linear_fc2.bias'
        }
    )
)

### unwrapped_model / model
GPTModel(
  (embedding): LanguageModelEmbedding(
    (word_embeddings): VocabParallelEmbedding()
    (embedding_dropout): Dropout(p=0.0, inplace=False)
  )
  (rotary_pos_emb): RotaryEmbedding()
  (decoder): TransformerBlock(
    (layers): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear(in_features=3072, out_features=4096, bias=False, TP=4)
          (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): IdentityOp()
        (mlp): MLP(
          (linear_fc1): TELayerNormColumnParallelLinear(in_features=4096, out_features=5472, bias=False, TP=4)
          (linear_fc2): TERowParallelLinear(in_features=2736, out_features=4096, bias=False, TP=4)
        )
      )
      (1-22): 22 x TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear(in_features=3072, out_features=4096, bias=False, TP=4)
          (linear_qkv): TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): RMSNorm()
        (mlp): MoELayer(
          (router): TopKRouter()
          (experts): TEGroupedMLP(
            (linear_fc1): TEColumnParallelGroupedLinear()
            (linear_fc2): TERowParallelGroupedLinear()
          )
          (shared_experts): SharedExpertMLP(
            (linear_fc1): TEColumnParallelLinear(in_features=4096, out_features=704, bias=False, TP=4)
            (linear_fc2): TERowParallelLinear(in_features=352, out_features=4096, bias=False, TP=4)
          )
        )
      )
    )
  )
)


### lora_kwargs
{
    'r': 16, 
    'target_modules': [
        'linear_proj', 
        'linear_fc1', 
        'linear_qkv', 
        'linear_fc2'
    ], 
    'lora_alpha': 32, 
    'lora_dropout': 0.05, 
    'bias': 'none', 
    'modules_to_save': [], 
    'use_rslora': False
}

### lora_config
LoraConfig(
  task_type='CAUSAL_LM', 
  peft_type=<PeftType.LORA: 'LORA'>, 
  auto_mapping=None, 
  base_model_name_or_path=None, 
  revision=None, 
  inference_mode=False, 
  r=16, 
  target_modules={
    'linear_proj', 
    'linear_qkv', 
    'linear_fc2', 
    'linear_fc1'
  }, 
  exclude_modules=None, 
  lora_alpha=32, 
  lora_dropout=0.05, 
  fan_in_fan_out=False, 
  bias='none', 
  use_rslora=False, 
  modules_to_save=[], 
  init_lora_weights=True, 
  layers_to_transform=None, 
  layers_pattern=None, 
  rank_pattern={}, 
  alpha_pattern={}, 
  megatron_config=None, 
  megatron_core='megatron.core', 
  trainable_token_indices=None, 
  loftq_config={}, 
  eva_config=None, 
  corda_config=None, 
  use_dora=False, 
  use_qalora=False, 
  qalora_group_size=16, 
  layer_replication=None, 
  runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), 
  lora_bias=False, 
  lora_dtype=None, 
  lorap_lr_ratio=None, 
  lorap_emb_lr=1e-06
)


### MODEL_TYPE_TO_PEFT_MODEL_MAPPING
{
    'SEQ_CLS': <class 'peft.peft_model.PeftModelForSequenceClassification'>, 
    'SEQ_2_SEQ_LM': <class 'peft.peft_model.PeftModelForSeq2SeqLM'>, 
    'CAUSAL_LM': <class 'peft.peft_model.PeftModelForCausalLM'>, 
    'TOKEN_CLS': <class 'peft.peft_model.PeftModelForTokenClassification'>, 
    'QUESTION_ANS': <class 'peft.peft_model.PeftModelForQuestionAnswering'>, 
    'FEATURE_EXTRACTION': <class 'peft.peft_model.PeftModelForFeatureExtraction'>
}


### LoRA 后的 model_module
GPTModel(
  (embedding): LanguageModelEmbedding(
    (word_embeddings): VocabParallelEmbedding()
    (embedding_dropout): Dropout(p=0.0, inplace=False)
  )
  (rotary_pos_emb): RotaryEmbedding()
  (decoder): TransformerBlock(
    (layers): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): LoraParallelLinear(
            (base_layer): TERowParallelLinear(in_features=3072, out_features=4096, bias=False, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TERowParallelLinear(in_features=3072, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TELinear(in_features=16, out_features=4096, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (linear_qkv): LoraParallelLinear(
            (base_layer): TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TELinear(in_features=4096, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TEColumnParallelLinear(in_features=16, out_features=3584, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): IdentityOp()
        (mlp): MLP(
          (linear_fc1): LoraParallelLinear(
            (base_layer): TELayerNormColumnParallelLinear(in_features=4096, out_features=5472, bias=False, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TELinear(in_features=4096, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TEColumnParallelLinear(in_features=16, out_features=5472, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (linear_fc2): LoraParallelLinear(
            (base_layer): TERowParallelLinear(in_features=2736, out_features=4096, bias=False, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TERowParallelLinear(in_features=2736, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TELinear(in_features=16, out_features=4096, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
        )
      )
      (1-22): 22 x TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.0, inplace=False)
            )
          )
          (linear_proj): LoraParallelLinear(
            (base_layer): TERowParallelLinear(in_features=3072, out_features=4096, bias=False, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TERowParallelLinear(in_features=3072, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TELinear(in_features=16, out_features=4096, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (linear_qkv): LoraParallelLinear(
            (base_layer): TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)
            (lora_dropout): ModuleDict(
              (default): Dropout(p=0.05, inplace=False)
            )
            (lora_A): ModuleDict(
              (default): TELinear(in_features=4096, out_features=16, bias=False, TP=4)
            )
            (lora_B): ModuleDict(
              (default): TEColumnParallelLinear(in_features=16, out_features=3584, bias=False, TP=4)
            )
            (lora_embedding_A): ParameterDict()
            (lora_embedding_B): ParameterDict()
            (lora_magnitude_vector): ModuleDict()
          )
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): RMSNorm()
        (mlp): MoELayer(
          (router): TopKRouter()
          (experts): TEGroupedMLP(
            (linear_fc1): LoraParallelLinear(
              (base_layer): TEColumnParallelGroupedLinear()
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): TEGroupedLinear()
              )
              (lora_B): ModuleDict(
                (default): TEColumnParallelGroupedLinear()
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (linear_fc2): LoraParallelLinear(
              (base_layer): TERowParallelGroupedLinear()
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): TERowParallelGroupedLinear()
              )
              (lora_B): ModuleDict(
                (default): TEGroupedLinear()
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
          )
          (shared_experts): SharedExpertMLP(
            (linear_fc1): LoraParallelLinear(
              (base_layer): TEColumnParallelLinear(in_features=4096, out_features=704, bias=False, TP=4)
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): TELinear(in_features=4096, out_features=16, bias=False, TP=4)
              )
              (lora_B): ModuleDict(
                (default): TEColumnParallelLinear(in_features=16, out_features=704, bias=False, TP=4)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
            (linear_fc2): LoraParallelLinear(
              (base_layer): TERowParallelLinear(in_features=352, out_features=4096, bias=False, TP=4)
              (lora_dropout): ModuleDict(
                (default): Dropout(p=0.05, inplace=False)
              )
              (lora_A): ModuleDict(
                (default): TERowParallelLinear(in_features=352, out_features=16, bias=False, TP=4)
              )
              (lora_B): ModuleDict(
                (default): TELinear(in_features=16, out_features=4096, bias=False, TP=4)
              )
              (lora_embedding_A): ParameterDict()
              (lora_embedding_B): ParameterDict()
              (lora_magnitude_vector): ModuleDict()
            )
          )
        )
      )
    )
  )
)


### ddp_config
DistributedDataParallelConfig(
  grad_reduce_in_fp32=True, 
  overlap_grad_reduce=False, 
  overlap_param_gather=False, 
  align_param_gather=False, 
  use_distributed_optimizer=True, 
  num_distributed_optimizer_instances=1, 
  check_for_nan_in_grad=True, 
  check_for_large_grads=False, 
  bucket_size=None, 
  pad_buckets_for_high_nccl_busbw=False, 
  average_in_collective=False, 
  fp8_param_gather=False, 
  reuse_grad_buf_for_mxfp8_param_ag=False, 
  use_custom_fsdp=False, 
  data_parallel_sharding_strategy='no_shard', 
  gradient_reduce_div_fusion=True, 
  suggested_communication_unit_size=None, 
  preserve_fp32_weights=True, 
  keep_fp8_transpose_cache_when_using_custom_fsdp=False, 
  nccl_ub=False, 
  fsdp_double_buffer=False
)



### LoRA 前
TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)

### LoRA 后
LoraParallelLinear(
  (base_layer): TELayerNormColumnParallelLinear(in_features=4096, out_features=3584, bias=True, TP=4)
  (lora_dropout): ModuleDict(
    (default): Dropout(p=0.05, inplace=False)
  )
  (lora_A): ModuleDict(
    (default): TELinear(in_features=4096, out_features=16, bias=False, TP=4)
  )
  (lora_B): ModuleDict(
    (default): TEColumnParallelLinear(in_features=16, out_features=3584, bias=False, TP=4)
  )
  (lora_embedding_A): ParameterDict()
  (lora_embedding_B): ParameterDict()
  (lora_magnitude_vector): ModuleDict()
)

### FP16 包裹后的 model
[Float16Module(
  (module): GPTModel(
    (em...
        )
      )
    )
  )
)]


### _MODEL_PARALLEL_ATTRIBUTE_DEFAULTS
{
    "tensor_model_parallel": False,
    "partition_dim": -1,
    "partition_stride": 1,
}


### 分布式相关参数 DistributedDataParallelConfig
DistributedDataParallelConfig(
  grad_reduce_in_fp32=True, 
  overlap_grad_reduce=False, 
  overlap_param_gather=False, 
  align_param_gather=False, 
  use_distributed_optimizer=True, 
  num_distributed_optimizer_instances=1, 
  check_for_nan_in_grad=True, 
  check_for_large_grads=False, 
  bucket_size=None, 
  pad_buckets_for_high_nccl_busbw=False, 
  average_in_collective=False, 
  fp8_param_gather=False, 
  reuse_grad_buf_for_mxfp8_param_ag=False, 
  use_custom_fsdp=False, 
  data_parallel_sharding_strategy='no_shard', 
  gradient_reduce_div_fusion=True, 
  suggested_communication_unit_size=None, 
  preserve_fp32_weights=True, 
  keep_fp8_transpose_cache_when_using_custom_fsdp=False, 
  nccl_ub=False, 
  fsdp_double_buffer=False
)


### DDP 包裹后的 model
[DistributedDataParallel(
  (module): Float16Module(
    (module): GPTModel(
      (em...
          )
        )
      )
    )
  )
)]


### 优化器配置 OptimizerConfig
OptimizerConfig(
  optimizer='adam', 
  lr=0.0001, 
  min_lr=1e-05, 
  decoupled_lr=None, 
  decoupled_min_lr=None, 
  weight_decay=0.1, 
  fp8_recipe='delayed', 
  fp16=False, 
  bf16=True, 
  reuse_grad_buf_for_mxfp8_param_ag=False, 
  params_dtype=torch.bfloat16, 
  use_precision_aware_optimizer=False, 
  store_param_remainders=True, 
  main_grads_dtype=torch.float32, 
  main_params_dtype=torch.float32, 
  exp_avg_dtype=torch.float32, 
  exp_avg_sq_dtype=torch.float32, 
  loss_scale=None, 
  initial_loss_scale=4294967296, 
  min_loss_scale=1.0, 
  loss_scale_window=1000, 
  hysteresis=2, 
  adam_beta1=0.9, 
  adam_beta2=0.95, 
  adam_eps=1e-08, 
  sgd_momentum=0.9, 
  use_distributed_optimizer=True, 
  overlap_param_gather_with_optimizer_step=False, 
  optimizer_cpu_offload=False, 
  optimizer_offload_fraction=1.0, 
  use_torch_optimizer_for_cpu_offload=False, 
  overlap_cpu_optimizer_d2h_h2d=False, 
  pin_cpu_grads=True, 
  pin_cpu_params=True, 
  clip_grad=1.0, 
  log_num_zeros_in_grad=False, 
  barrier_with_L1_time=True, 
  timers=<megatron.core.timers.Timers object at 0x7fdae0573bd0>, 
  config_logger_dir=''
)


### 优化器参数
{
  'params': [{...}], 
  'lr': 0.0001, 
  'weight_decay': 0.1, 
  'betas': (0.9, 0.95), 
  'eps': 1e-08
}


### optimizer
FusedAdam (
Parameter Group 0
    betas: (0.9, 0.95)
    bias_correction: True
    eps: 1e-08
    is_decoupled_lr: False
    is_expert_parallel: False
    lr: 0.0001
    lr_mult: 1.0
    max_lr: 0.0001
    min_lr: 1e-05
    wd_mult: 1.0
    weight_decay: 0.1
)


### common.pt
`state_dict = torch.load('/models/ZhipuAI/GLM-4.5-Air-mcore/iter_0000001/common.pt', map_location='cpu', weights_only=False)`
state_dict:
{
  'args': Namespace(
    num_layers=46, encoder_num_layers=46, decoder_num_layers=None, hidden_size=4096, ffn_hidden_size=10944, num_attention_heads=96, attention_backend=<AttnBackend.unfused: 3>, kv_channels=128, group_query_attention=True, num_query_groups=8, max_position_embeddings=131072, position_embedding_type='rope', relative_attention_num_buckets=32, relative_attention_max_distance=128, use_rotary_position_embeddings=False, rotary_base=1000000, rotary_percent=1.0, rotary_interleaved=False, rotary_seq_len_interpolation_factor=None, use_rope_scaling=False, rope_scaling_factor=8.0, no_rope_freq=None, add_position_embedding=True, mrope_section=None, make_vocab_size_divisible_by=128, normalization='RMSNorm', norm_epsilon=1e-05, apply_layernorm_1p=False, apply_residual_connection_post_layernorm=False, openai_gelu=False, squared_relu=False, swiglu=True, onnx_safe=None, bert_binary_head=True, untie_embeddings_and_output_weights=True, multi_latent_attention=False, mtp_num_layers=None, mtp_loss_scaling_factor=0.1, attention_dropout=0.0, hidden_dropout=0.0, weight_decay=0.1, start_weight_decay=0.1, end_weight_decay=0.1, weight_decay_incr_style='constant', clip_grad=1.0, adam_beta1=0.9, adam_beta2=0.95, adam_eps=1e-08, sgd_momentum=0.9, micro_batch_size=1, global_batch_size=16, rampup_batch_size=None, decrease_batch_size_if_needed=False, recompute_granularity='selective', check_for_nan_in_loss_and_grad=True, check_for_spiky_loss=False, check_for_large_grads=False, distribute_saved_activations=False, recompute_method=None, recompute_num_layers=None, recompute_modules=['core_attn'], clone_scatter_output_in_embedding=True, profile=False, profile_step_start=10, profile_step_end=12, iterations_to_skip=[], result_rejected_tracker_filename=None, enable_gloo_process_groups=True, use_pytorch_profiler=False, profile_ranks=[0], record_memory_history=False, memory_snapshot_path='snapshot.pickle', tp_comm_overlap=False, tp_comm_overlap_cfg=None, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_bulk_dgrad=True, tp_comm_bulk_wgrad=True, tp_comm_bootstrap_backend='nccl', use_cpu_initialization=True, empty_unused_memory_level=0, deterministic_mode=False, check_weight_hash_across_dp_replicas_interval=None, calculate_per_token_loss=True, train_sync_interval=None, train_iters=None, train_samples=None, log_interval=5, exit_interval=None, exit_duration_in_mins=None, exit_signal_handler=False, tensorboard_dir='/models/ZhipuAI/GLM-4.5-Air-mcore/runs', masked_softmax_fusion=True, bias_gelu_fusion=False, bias_swiglu_fusion=True, bias_dropout_fusion=True, apply_rope_fusion=True, cross_entropy_loss_fusion=False, cross_entropy_fusion_impl='native', use_flash_attn=False, add_bias_linear=False, add_qkv_bias=True, optimizer='adam', optimizer_cpu_offload=False, optimizer_offload_fraction=1.0, use_torch_optimizer_for_cpu_offload=False, overlap_cpu_optimizer_d2h_h2d=False, pin_cpu_grads=True, pin_cpu_params=True, dataloader_type='cyclic', async_tensor_model_parallel_allreduce=True, no_persist_layer_norm=False, sequence_parallel=False, gradient_accumulation_fusion=True, deprecated_use_mcore_models=False, use_legacy_models=False, manual_gc=False, manual_gc_interval=0, manual_gc_eval=True, tp_comm_split_ag=True, tp_comm_split_rs=True, pipeline_model_parallel_comm_backend=None, high_priority_stream_groups=[], seed=42, data_parallel_random_init=False, init_method_std=0.02, init_method_xavier_uniform=False, lr=1e-05, lr_decay_style='cosine', lr_wsd_decay_style='exponential', lr_decay_iters=None, lr_decay_samples=None, lr_wsd_decay_samples=None, lr_wsd_decay_iters=None, lr_warmup_fraction=None, lr_warmup_iters=0, lr_warmup_samples=0, lr_warmup_init=0.0, min_lr=0.0, override_opt_param_scheduler=False, use_checkpoint_opt_param_scheduler=False, decoupled_lr=None, decoupled_min_lr=None, save='/models/ZhipuAI/GLM-4.5-Air-mcore', save_interval=500, no_save_optim=True, no_save_rng=True, load=None, no_load_optim=True, no_load_rng=True, non_persistent_save_interval=None, non_persistent_ckpt_type=None, non_persistent_global_ckpt_dir=None, non_persistent_local_ckpt_dir=None, non_persistent_local_ckpt_algo='fully_parallel', finetune=False, pretrained_checkpoint=None, ckpt_step=None, perform_initialization=False, use_checkpoint_args=False, use_mp_args_from_checkpoint_args=False, use_tokenizer_model_from_checkpoint_args=True, exit_on_missing_checkpoint=True, use_dist_ckpt_deprecated=False, use_persistent_ckpt_worker=False, auto_detect_ckpt_format=True, dist_ckpt_format_deprecated=None, ckpt_format='torch_dist', ckpt_convert_format=None, ckpt_convert_save=None, ckpt_convert_update_legacy_dist_opt_format=False, ckpt_fully_parallel_save_deprecated=False, ckpt_fully_parallel_save=True, async_save=None, ckpt_fully_parallel_load=False, ckpt_assume_constant_structure=False, dist_ckpt_strictness='assume_ok_unexpected', load_model_opt_format=False, fp16=False, bf16=True, grad_reduce_in_bf16=False, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, fp32_residual_connection=False, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=True, accumulate_allreduce_grads_in_fp32=True, fp16_lm_cross_entropy=False, disable_bf16_reduced_precision_matmul=False, reuse_grad_buf_for_mxfp8_param_ag=False, tensor_model_parallel_size=1, encoder_tensor_model_parallel_size=0, pipeline_model_parallel_size=1, encoder_pipeline_model_parallel_size=0, pipeline_model_parallel_split_rank=None, decoder_first_pipeline_num_layers=None, decoder_last_pipeline_num_layers=None, pipeline_model_parallel_layout=None, num_layers_per_virtual_pipeline_stage=None, num_virtual_stages_per_pipeline_rank=None, microbatch_group_size_per_vp_stage=None, overlap_p2p_comm=False, overlap_p2p_comm_warmup_flush=False, distributed_backend='nccl', distributed_timeout_minutes=300000, overlap_grad_reduce=False, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, align_grad_reduce=True, ddp_num_buckets=None, ddp_bucket_size=None, ddp_pad_buckets_for_high_nccl_busbw=False, ddp_average_in_collective=False, overlap_param_gather=False, overlap_param_gather_with_optimizer_step=False, align_param_gather=False, scatter_gather_tensors_in_pipeline=True, use_ring_exchange_p2p=False, local_rank=0, lazy_mpu_init=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, use_distributed_optimizer=True, nccl_ub=False, use_sharp=False, use_custom_fsdp=False, init_model_with_meta_device=False, data_parallel_sharding_strategy='no_shard', gradient_reduce_div_fusion=True, fsdp_double_buffer=False, suggested_communication_unit_size=None, keep_fp8_transpose_cache_when_using_custom_fsdp=False, num_distributed_optimizer_instances=1, use_torch_fsdp2=False, torch_fsdp2_reshard_after_forward=True, context_parallel_size=1, cp_comm_type=['p2p'], hierarchical_context_parallel_sizes=None, nccl_communicator_config_path=None, use_tp_pp_dp_mapping=False, replication=False, replication_jump=None, replication_factor=2, eval_iters=-1, eval_interval=500, test_mode=False, skip_train=False, data_path=None, split=None, train_data_path=None, valid_data_path=None, test_data_path=None, data_args_path=None, per_split_data_args_path=None, data_cache_path=None, mmap_bin_files=True, mock_data=False, seq_length=131072, encoder_seq_length=131072, decoder_seq_length=None, retriever_seq_length=256, sample_rate=1.0, mask_prob=0.15, short_seq_prob=0.1, num_workers=4, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, create_attention_mask_in_dataloader=True, num_dataset_builder_threads=1, object_storage_cache_path=None, mid_level_dataset_surplus=0.005, vocab_size=None, vocab_file=None, merge_file=None, vocab_extra_ids=0, tokenizer_type=None, tokenizer_model=None, tiktoken_pattern=None, tiktoken_num_special_tokens=1000, tiktoken_special_tokens=None, adlr_autoresume=False, adlr_autoresume_interval=1000, ict_head_size=None, biencoder_projection_dim=0, biencoder_shared_query_context_model=False, ict_load=None, bert_load=None, titles_data_path=None, query_in_block_prob=0.1, use_one_sent_docs=False, evidence_data_path=None, retriever_report_topk_accuracies=[], retriever_score_scaling=False, block_data_path=None, embedding_path=None, indexer_batch_size=128, indexer_log_interval=1000, num_classes=1000, img_h=224, img_w=224, num_channels=3, patch_dim=16, classes_fraction=1.0, data_per_class_fraction=1.0, data_sharding=True, head_lr_mult=1.0, vision_pretraining=False, vision_pretraining_type='classify', vision_backbone_type='vit', swin_backbone_type='tiny', mask_type='random', mask_factor=1.0, iter_per_epoch=1250, dino_local_img_size=96, dino_local_crops_number=10, dino_head_hidden_size=2048, dino_bottleneck_size=256, dino_freeze_last_layer=1, dino_norm_last_layer=False, dino_warmup_teacher_temp=0.04, dino_teacher_temp=0.07, dino_warmup_teacher_temp_epochs=30, qk_layernorm=False, qk_l2_norm=False, expert_model_parallel_size=1, expert_tensor_parallel_size=1, num_experts=128, moe_layer_freq=[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], moe_ffn_hidden_size=1408, moe_shared_expert_intermediate_size=1408, moe_shared_expert_overlap=False, moe_grouped_gemm=True, moe_use_legacy_grouped_gemm=False, moe_layer_recompute=False, moe_extended_tp=False, moe_use_upcycling=False, moe_router_load_balancing_type='aux_loss', moe_router_dtype='fp32', moe_router_score_function='sigmoid', moe_router_topk=8, moe_router_pre_softmax=False, moe_router_num_groups=None, moe_router_group_topk=None, moe_router_topk_scaling_factor=1.0, moe_router_enable_expert_bias=True, moe_router_bias_update_rate=0.001, moe_router_force_load_balancing=False, moe_router_padding_for_fp8=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_per_layer_logging=False, moe_token_dispatcher_type='alltoall', moe_enable_deepep=False, moe_deepep_num_sms=20, moe_permute_fusion=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_apply_probs_on_input=False, delay_wgrad_compute=False, moe_upcycling_granularity=1, q_lora_rank=None, kv_lora_rank=32, qk_head_dim=128, qk_pos_emb_head_dim=64, v_head_dim=128, rotary_scaling_factor=1.0, mscale=1.0, mscale_all_dim=1.0, heterogeneous_layers_config_path=None, heterogeneous_layers_config_encoded_json=None, log_params_norm=False, log_num_zeros_in_grad=False, log_throughput=False, log_progress=False, timing_log_level=0, log_energy=False, barrier_with_L1_time=True, timing_log_option='minmax', tensorboard_log_interval=1, tensorboard_queue_size=50, log_timers_to_tensorboard=True, log_loss_scale_to_tensorboard=True, log_validation_ppl_to_tensorboard=True, log_memory_to_tensorboard=True, log_world_size_to_tensorboard=False, wandb_project='', wandb_exp_name='', wandb_save_dir='', logging_level=None, log_straggler=False, disable_straggler_on_startup=False, straggler_ctrlr_port=65535, straggler_minmax_count=1, run_workload_inspector_server=False, inference_batch_times_seqlen_threshold=-1, max_tokens_to_oom=12000, output_bert_embeddings=False, bert_embedder_type='megatron', flash_decode=False, enable_cuda_graph=False, cuda_graph_warmup_steps=3, external_cuda_graph=False, cuda_graph_scope='full', inference_max_batch_size=8, inference_max_seq_length=2560, inference_dynamic_batching=False, inference_dynamic_batching_buffer_size_gb=40.0, inference_dynamic_batching_chunk_size=256, inference_dynamic_batching_buffer_guaranteed_fraction=0.2, inference_dynamic_batching_buffer_overflow_factor=None, inference_dynamic_batching_max_requests_override=None, inference_dynamic_batching_max_tokens_override=None, symmetric_ar_type=None, nccl_all_reduce_for_prefill=False, mlp_chunks_for_prefill=1, fp8=None, fp8_recipe='delayed', fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1024, fp8_amax_compute_algo='max', fp8_wgrad=True, transformer_impl='transformer_engine', fp8_param_gather=False, first_last_layers_bf16=False, num_layers_at_start_in_bf16=1, num_layers_at_end_in_bf16=1, te_rng_tracker=False, inference_rng_tracker=False, retro_project_dir=None, retro_add_retriever=False, retro_cyclic_train_iters=None, retro_encoder_layers=2, retro_encoder_hidden_dropout=0.1, retro_encoder_attention_dropout=0.1, retro_num_neighbors=2, retro_num_retrieved_chunks=2, retro_attention_gate=1, retro_verify_neighbor_count=True, enable_experimental=False, spec=None, hybrid_attention_ratio=0.0, hybrid_mlp_ratio=0.0, hybrid_override_pattern=None, mamba_state_dim=128, mamba_head_dim=64, mamba_num_groups=8, mamba_num_heads=None, is_hybrid_model=False, disable_mamba_mem_eff_path=False, yaml_cfg=None, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, enable_one_logger=True, one_logger_project='megatron-lm', one_logger_run_name=None, one_logger_async=False, app_tag_run_name=None, app_tag_run_version='0.0.0', inprocess_restart=False, inprocess_max_iterations=None, inprocess_monitor_thread_interval=1.0, inprocess_monitor_process_interval=1.0, inprocess_progress_watchdog_interval=1.0, inprocess_heartbeat_interval=30, inprocess_soft_timeout=60, inprocess_hard_timeout=90, inprocess_heartbeat_timeout=60, inprocess_barrier_timeout=120, inprocess_completion_timeout=120, inprocess_last_call_wait=1, inprocess_termination_grace_time=1, inprocess_granularity='node', inprocess_active_world_size=1, inprocess_empty_cuda_cache=False, enable_ft_package=False, calc_ft_timeouts=False, config_logger_dir='', error_injection_rate=0, error_injection_type='transient_error', rerun_mode='disabled', enable_msc=True, kitchen_config_file=None, kitchen_recipe_number=None, sft=False, sft_tokenizer_prompt_format='nemotron-h-aligned', rank=0, world_size=1, use_dist_ckpt=True, transformer_pipeline_model_parallel_size=1, data_parallel_size=1, train_type='full', freeze_parameters=[], freeze_parameters_regex=None, freeze_parameters_ratio=0.0, trainable_parameters=[], trainable_parameters_regex=None, adapter_load=None, target_modules=['all-linear'], target_regex=None, modules_to_save=[], lora_rank=8, lora_alpha=32, lora_dropout=0.05, lora_bias='none', lora_dtype=None, use_rslora=False, ref_load=None, beta=0.1, rpo_alpha=None, reference_free=False, label_smoothing=0.0, f_divergence_type='reverse_kl', loss_type='sigmoid', padded_vocab_size=151552, initialize_embedding=False, rope_scaling=None, torch_dtype=torch.bfloat16, padding_free=True, mlp_padding_free=False, dataloader_persistent_workers=True, dataloader_prefetch_factor=10, architectures='Glm4MoeForCausalLM', max_epochs=None, enable_dft_loss=False, original_max_position_embeddings=None, partial_rotary_factor=0.5, use_shared_expert_gate=False, virtual_pipeline_model_parallel_size=None, params_dtype=torch.bfloat16, consumed_train_samples=0, skipped_train_samples=0, consumed_valid_samples=0, variable_seq_lengths=False
  ),
  'checkpoint_version': 3.0,
  'iteration': 1,
  'rerun_state_machine': {
    'mode': <RerunMode.DISABLED: 'disabled'>
  },
  'num_floating_point_operations_so_far': 0,
  'content_metadata': {
    'distrib_optim_sharding_type': 'fully_sharded_model_space'
  }
}


### sharded_sd_metadata
{'distrib_optim_sharding_type': 'fully_sharded_model_space'}

### model_sd_kwargs
{'metadata': {'distrib_optim_sharding_type': 'fully_sharded_model_space'}}



### state_dict
{
  'args': Namespace(
    num_layers=46, encoder_num_layers=46, decoder_num_layers=None, hidden_size=...lid_samples=0, variable_seq_lengths=False
  ), 
  'checkpoint_version': 3.0, 
  'model': {
    'embedding.word_embeddings.weight': ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_sha...shape_mismatch=True, flattened_range=None), 
    'decoder.layers.0.self_attention.core_attention._extra_state': ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data... global_offset=(0,), replica_id=(0, 2, 0)), 
    'decoder.layers.0.self_attention.linear_proj.base_layer.weight': ShardedTensor(key='decoder.layers.0.self_attention.linear_proj.base_layer.weight', dt...hape_mismatch=False, flattened_range=None), 
    'decoder.layers.0.self_attention.linear_proj.base_layer._extra_state': ShardedObject(key='decoder.layers.0.self_attention.linear_proj.base_layer._extra_stat... global_offset=(0,), replica_id=(0, 2, 0)), 
    ...,
    'decoder.layers.0.mlp.linear_fc1.base_layer.weight': ShardedTensorFactory(key='decoder.layers.0.mlp.linear_fc1.base_layer.weight', data=Pa...eplica_id=(0, 0, 0), flattened_range=None), 
    'decoder.layers.0.mlp.linear_fc1.base_layer._extra_state': ShardedObject(key='decoder.layers.0.mlp.linear_fc1.base_layer._extra_state', data=ten... global_offset=(0,), replica_id=(0, 2, 0)), ...
  }, 
  'rerun_state_machine': None
}

### common_state_dict
{
  'args': Namespace(
    num_layers=46, encoder_num_layers=46, decoder_num_layers=None, hidden_size=...lid_samples=0, variable_seq_lengths=False
  ), 
  'checkpoint_version': 3.0, 
  'iteration': 1, 
  'rerun_state_machine': {'mode': <RerunMode.DISABLED: 'disabled'>}, 
  'num_floating_point_operations_so_far': 0, 
  'content_metadata': {
    'distrib_optim_sharding_type': 'fully_sharded_model_space'
  }
}


### global_metadata
GPU0：
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(0, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 0)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 0))
]

GPU1:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(37888, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 1, 0)),
  ...,
  ShardedTensor(key='decoder.layers.22.mlp.shared_experts.linear_fc2.weight', dtype=torch.bfloat16, local_shape=(4096, 352), global_shape=(4096, 1408), global_offset=(0, 352), axis_fragmentations=(1, 4), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None),
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 1, 0))
]

GPU2：
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(75776, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 2, 0)),
  ShardedTensor(key='decoder.layers.0.self_attention.linear_proj.weight', dtype=torch.bfloat16, local_shape=(4096, 3072), global_shape=(4096, 12288), global_offset=(0, 6144), axis_fragmentations=(1, 4), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 2, 0))
]

GPU3:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(113664, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 3, 0)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 3, 0))
]

GPU4:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(0, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 1)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 1))
]

GPU5:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(37888, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 1, 1)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 1, 1))
]

GPU6:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(75776, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 2, 1)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 2, 1))
]

GPU7:
[
  ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(113664, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None),
  ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 3, 1)),
  ...,
  ShardedObject(key='decoder.layers.22.mlp.shared_experts.linear_fc2._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 3, 1))
]

GPU8:
[
  ShardedObject(key='decoder.layers.23.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 0)),
  ShardedTensor(key='decoder.layers.23.self_attention.linear_proj.weight', dtype=torch.bfloat16, local_shape=(4096, 3072), global_shape=(4096, 12288), global_offset=(0, 0), axis_fragmentations=(1, 4), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None),
  ...,
  ShardedTensor(key='output_layer.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(0, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None)
]

...

GPU15:
[
  ShardedObject(key='decoder.layers.23.self_attention.core_attention._extra_state', data=None, global_shape=(1,), global_offset=(0,), replica_id=(0, 3, 1)),
  ShardedTensor(key='decoder.layers.23.self_attention.linear_proj.weight', dtype=torch.bfloat16, local_shape=(4096, 3072), global_shape=(4096, 12288), global_offset=(0, 9216), axis_fragmentations=(1, 4), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None),
  ...,
  ShardedTensor(key='output_layer.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(113664, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 1), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None)
]


### sharded_state_dict
{
  'embedding.word_embeddings.weight': [ShardedTensor(key='embedding.word_embeddings.weight', dtype=torch.bfloat16, local_shape=(37888, 4096), global_shape=(151552, 4096), global_offset=(0, 0), axis_fragmentations=(4, 1), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=True, flattened_range=None)],
  'decoder.layers.0.self_attention.core_attention._extra_state': [ShardedObject(key='decoder.layers.0.self_attention.core_attention._extra_state', data=tensor([], dtype=torch.uint8), global_shape=(1,), global_offset=(0,), replica_id=(0, 0, 0))],
  'decoder.layers.0.self_attention.linear_proj.weight': [ShardedTensor(key='decoder.layers.0.self_attention.linear_proj.weight', dtype=torch.bfloat16, local_shape=(4096, 3072), global_shape=(4096, 12288), global_offset=(0, 0), axis_fragmentations=(1, 4), replica_id=(0, 0, 0), prepend_axis_num=0, allow_shape_mismatch=False, flattened_range=None)]
}

### pyt_state_dict
GPU0:
{
  'embedding.word_embeddings.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[37888, 4096], placement=rank:0/cuda), 
        ShardMetadata(shard_offsets=[37888, 0], shard_sizes=[37888, 4096], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[75776, 0], shard_sizes=[37888, 4096], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[113664, 0], shard_sizes=[37888, 4096], placement=rank:1/cuda)
      ], 
      size=torch.Size([151552, 4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  ),
  'decoder.layers.0.self_attention.core_attention._extra_state/shard_0_1': <_io.BytesIO object at 0x7f88d035cdb0>,
  'decoder.layers.0.self_attention.linear_proj.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 3072], placement=rank:0/cuda), 
        ShardMetadata(shard_offsets=[0, 3072], shard_sizes=[4096, 3072], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[0, 6144], shard_sizes=[4096, 3072], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[0, 9216], shard_sizes=[4096, 3072], placement=rank:1/cuda)
      ], 
      size=torch.Size([4096, 12288]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  ),
  'decoder.layers.0.self_attention.linear_qkv.layer_norm_weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=(0,), shard_sizes=torch.Size([4096]), placement=rank:0/cuda)
      ], 
      size=torch.Size([4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  ),
  ...
}

GPU1:
{
  'embedding.word_embeddings.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[37888, 4096], placement=rank:2/cuda), 
        ShardMetadata(shard_offsets=[37888, 0], shard_sizes=[37888, 4096], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[75776, 0], shard_sizes=[37888, 4096], placement=rank:2/cuda), 
        ShardMetadata(shard_offsets=[113664, 0], shard_sizes=[37888, 4096], placement=rank:2/cuda)
      ], 
      size=torch.Size([151552, 4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  ),
  'decoder.layers.0.self_attention.linear_proj.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[4096, 3072], placement=rank:2/cuda), 
        ShardMetadata(shard_offsets=[0, 3072], shard_sizes=[4096, 3072], placement=rank:1/cuda), 
        ShardMetadata(shard_offsets=[0, 6144], shard_sizes=[4096, 3072], placement=rank:2/cuda), 
        ShardMetadata(shard_offsets=[0, 9216], shard_sizes=[4096, 3072], placement=rank:2/cuda)
      ], 
      size=torch.Size([4096, 12288]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  ),
  ...
}

GPU2:
{
  'embedding.word_embeddings.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[37888, 4096], placement=rank:3/cuda), 
        ShardMetadata(shard_offsets=[37888, 0], shard_sizes=[37888, 4096], placement=rank:3/cuda), 
        ShardMetadata(shard_offsets=[75776, 0], shard_sizes=[37888, 4096], placement=rank:2/cuda), 
        ShardMetadata(shard_offsets=[113664, 0], shard_sizes=[37888, 4096], placement=rank:3/cuda)
      ], 
      size=torch.Size([151552, 4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  )
}

GPU3:
{
  'embedding.word_embeddings.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[37888, 4096], placement=rank:4/cuda), 
        ShardMetadata(shard_offsets=[37888, 0], shard_sizes=[37888, 4096], placement=rank:4/cuda), 
        ShardMetadata(shard_offsets=[75776, 0], shard_sizes=[37888, 4096], placement=rank:4/cuda), 
        ShardMetadata(shard_offsets=[113664, 0], shard_sizes=[37888, 4096], placement=rank:3/cuda)
      ], 
      size=torch.Size([151552, 4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  )
}

GPU7:
{
  'embedding.word_embeddings.weight': ShardedTensor(
    ShardedTensorMetadata(
      shards_metadata=[
        ShardMetadata(shard_offsets=[0, 0], shard_sizes=[37888, 4096], placement=rank:8/cuda), 
        ShardMetadata(shard_offsets=[37888, 0], shard_sizes=[37888, 4096], placement=rank:8/cuda), 
        ShardMetadata(shard_offsets=[75776, 0], shard_sizes=[37888, 4096], placement=rank:8/cuda), 
        ShardMetadata(shard_offsets=[113664, 0], shard_sizes=[37888, 4096], placement=rank:7/cuda)
      ], 
      size=torch.Size([151552, 4096]), 
      tensor_properties=TensorProperties(
        dtype=torch.bfloat16, layout=torch.strided, requires_grad=False, memory_format=torch.contiguous_format, pin_memory=False
      )
    )
  )
}


### mcore_state_dict
GPU7:
{
  'embedding.word_embeddings.weight': [tensor(
    [
      [ 0.0022,  0.0080,  0.0108,  ...,  0.0156, -0.0052, -0.0033],
      [-0.0317, -0.0175, -0.0181,  ..., -0.0012, -0.0023, -0.0032],
      [ 0.0096,  0.0186,  0.0099,  ...,  0.0046,  0.0128, -0.0116],
      ...,
      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
      [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]
    ],
    device='cuda:7', dtype=torch.bfloat16)] # shape: [37888, 4096]
}

### common_state_dict
{
  'args': Namespace(
    num_layers=46, encoder_num_layers=46, decoder_num_layers=None, hidden_size=...lid_samples=0, variable_seq_lengths=False
  ), 
  'checkpoint_version': 3.0, 
  'iteration': 1, 
  'rerun_state_machine': {'mode': <RerunMode.DISABLED: 'disabled'>}, 
  'num_floating_point_operations_so_far': 0, 
  'content_metadata': {'distrib_optim_sharding_type': 'fully_sharded_model_space'}, 
  'model': {
    'embedding.word_embeddings.weight': tensor(
      [[-0.0095, -0.0083,  0.0077,  ..., -0.0082, -0.0063, -0.0019],
        [ 0.006...    device='cuda:1', dtype=torch.bfloat16), 
    'decoder.layers.0.self_attention.core_attention._extra_state': tensor([], dtype=torch.uint8), 
    'decoder.layers.0.self_attention.linear_proj.weight': tensor(
      [[-0.0009, -0.0039, -0.0011,  ..., -0.0092,  0.0011,  0.0074],
        [ 0.000...    device='cuda:1', dtype=torch.bfloat16), 
    'decoder.layers.0.self_attention.linear_proj._extra_state': tensor([], dtype=torch.uint8), 
    'decoder.layers.0.self_attention.linear_qkv.layer_norm_weight': tensor([0.0037, 0.0064, 0.0032,  ..., 0.0092, 0.0123, 0.0082], device='cuda:1',
       dtype=torch.bfloat16), 
    'decoder.layers.0.self_attention.linear_qkv.weight': tensor(
      [[ 1.0223e-03, -7.4005e-04, -3.7842e-03,  ..., -1.2665e-03,
         -1.9379e-...]], device='cuda:1', dtype=torch.bfloat16), 
      'decoder.layers.0.self_attention.linear_qkv.bias': tensor([-1.9141e-01, -1.4160e-01,  5.0391e-01,  ..., -1.6403e-04,
         1.1778e-04...4], device='cuda:1', dtype=torch.bfloat16), 
      'decoder.layers.0.self_attention.linear_qkv._extra_state': tensor([], dtype=torch.uint8), 
      'decoder.layers.0.mlp.linear_fc1.layer_norm_weight': tensor([0.0693, 0.0554, 0.0317,  ..., 0.0854, 0.0894, 0.0669], device='cuda:1',
       dtype=torch.bfloat16), 
    'decoder.layers.0.mlp.linear_fc1.weight': tensor(
      [[ 7.5531e-04, -6.7234e-05,  2.9907e-03,  ...,  5.4932e-03,
         -3.6865e-...]], device='cuda:1', dtype=torch.bfloat16), 
      'decoder.layers.0.mlp.linear_fc1._extra_state': tensor([], dtype=torch.uint8), 
      'decoder.layers.0.mlp.linear_fc2.weight': tensor(
        [[-0.0182,  0.0140, -0.0131,  ...,  0.0078, -0.0222,  0.0034],
        [-0.005...    device='cuda:1', dtype=torch.bfloat16), 
    'decoder.layers.0.mlp.linear_fc2._extra_state': tensor([], dtype=torch.uint8), 
    'decoder.layers.1.self_attention.core_attention._extra_state': tensor([], dtype=torch.uint8), 
    'decoder.layers.1.self_attention.linear_proj.weight': tensor(
      [[ 1.4709e-02, -6.2866e-03, -2.1696e-05,  ...,  7.3547e-03,
         -7.5684e-...]], device='cuda:1', dtype=torch.bfloat16), 
      'decoder.layers.1.self_attention.linear_proj._extra_state': tensor([], dtype=torch.uint8), 
      'decoder.layers.1.self_attention.linear_qkv.layer_norm_weight': tensor([0.0142, 0.0197, 0.0869,  ..., 0.0125, 0.0121, 0.0165], device='cuda:1',
       dtype=torch.bfloat16), 
    'decoder.layers.1.self_attention.linear_qkv.weight': tensor(
      [[-0.0248, -0.0079,  0.0413,  ...,  0.0120, -0.0032,  0.0062],
        [ 0.015...    device='cuda:1', dtype=torch.bfloat16), 
    'decoder.layers.1.self_attention.linear_qkv.bias': tensor([ 7.2266e-01, -4.2773e-01,  1.6504e-01,  ...,  1.5793e-03,
         5.3024e-04...3], device='cuda:1', dtype=torch.bfloat16), ...
  }
}